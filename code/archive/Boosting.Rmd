---
title: ""
author: "Mason Shihab"
date: "3/13/2021"
output:
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(readr)
library(readxl)
library(usdata)
library(tm)
library(stringr)
library(gbm)           # boosting
```



# Boosting (12 points for correctness; 3 points for presentation)

## Model tuning (4 points)

i. (2 points) Fit boosted tree models with interaction depths 1, 2, and 3. For each, use a shrinkage factor of 0.1, 1000 trees, and 5-fold cross-validation.

**Solution:**

```{r cache=TRUE}
set.seed(471) # for reproducibility (DO NOT CHANGE)
# TODO: Fit random forest with interaction depth 1

gbm_1 = gbm(theftrate ~ . -fips -state -county,
              distribution = "gaussian",
              n.trees = 1000,
              interaction.depth = 1,
              shrinkage = 0.1,
              cv.folds = 5,
              data = theft_train)

set.seed(471) # for reproducibility (DO NOT CHANGE)
# TODO: Fit random forest with interaction depth 2

gbm_2 = gbm(theftrate ~ . -fips -state -county,
              distribution = "gaussian",
              n.trees = 1000,
              interaction.depth = 2,
              shrinkage = 0.1,
              cv.folds = 5,
              data = theft_train)

set.seed(471) # for reproducibility (DO NOT CHANGE)
# TODO: Fit random forest with interaction depth 3


gbm_3 = gbm(theftrate ~ . -fips -state -county,
              distribution = "gaussian",
              n.trees = 1000,
              interaction.depth = 3,
              shrinkage = 0.1,
              cv.folds = 5,
              data = theft_train)
```



ii. (2 points) Plot the CV errors against the number of trees for each interaction depth. These three curves should be on the same plot with different colors. Also plot horizontal dashed lines at the minima of these three curves. What are the optimal interaction depth and number of trees?

**Solution:**

```{r}
ntrees = 1000
cv_errors = bind_rows(
  tibble(ntree = 1:ntrees, cv_err = gbm_1$cv.error, Depth = 1),
  tibble(ntree = 1:ntrees, cv_err = gbm_2$cv.error, Depth = 2),
  tibble(ntree = 1:ntrees, cv_err = gbm_3$cv.error, Depth = 3)
) %>% mutate(Depth = factor(Depth))

# plot CV errors

mins = cv_errors %>% group_by(Depth) %>% summarise(min_err = min(cv_err))

gbm.perf(gbm_3, plot.it = FALSE)
```


```{r deptherr, echo = TRUE, fig.width = 5, fig.height = 3, out.width = "100%", fig.align='center', fig.cap = "CV Error by Trees and Interaction Depth (with min error for each depth dashed)", fig.pos = "H"}
cv_errors %>%
  ggplot(aes(x = ntree, y = cv_err, colour = Depth)) +
  geom_line() + theme_bw() + 
  geom_hline(aes(yintercept = min_err, color = Depth), 
             data = mins, linetype = "dashed") + 
  labs(y = "CV Error", x = "Trees") + scale_y_log10()
```
**As we can see in Figure \@ref(fig:deptherr), Interaction Length 3 is best. We also see above that the optimal number of trees is 387.**

## Model interpretation (8 points)

i. (4 points) Print the first ten rows of the relative influence table for the optimal boosting model found above (using kable). What are the top three features? To what extent do these align with the top three features of the random forest trained above? 

**Solution:**

```{r relinf}
gbm_fit_optimal = gbm_3
optimal_num_trees = gbm.perf(gbm_1, plot.it = FALSE) 
summary(gbm_3, n.trees = optimal_num_trees, plotit = FALSE) %>% tibble() %>%
  head(10) 
```

**As we can see in Table \@ref(tab:relinf), the top three features are "!", "$", and "remove." These are also top-three features in the analogous table for random forests.**

ii. (4 points) Produce partial dependence plots for the top three features based on relative influence. Comment on the nature of the relationship with the response and whether it makes sense.

**Solution:**

```{r}
plot(gbm_3, i.var = "housing_density", 
     n.trees = optimal_num_trees)
```

```{r}
plot(gbm_3, i.var = "poor_fair_health", 
     n.trees = optimal_num_trees)

```

```{r}
plot(gbm_3, i.var = "pertrump", n.trees = 
       optimal_num_trees) 
```

```{r}
plot(gbm_3, i.var = "PctEmpFIRE", n.trees = 
       optimal_num_trees) 
```



```{r}
plot(gbm_3, i.var = "saversperpop", n.trees = 
       optimal_num_trees) 
```

```{r}
plot(gbm_3, i.var = "pop_density", n.trees = 
       optimal_num_trees) 
```

```{r}
plot(gbm_3, i.var = "unemp_bens_possible", n.trees = 
       optimal_num_trees) 
```

```{r}

theftdata = theftdata %>% mutate(weightedtrump = pertrump*pop_density/sum(pop_density))
  
 theftdata  %>%  ggplot(aes(x = pertrump, y = med_income)) + geom_smooth() + geom_point()
```
