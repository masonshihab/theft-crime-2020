<<<<<<< HEAD
filter(!is.na(Salary)) %>%   # remove NA values (in general not necessary)
mutate(Salary = log(Salary)) # log-transform the salary
Hitters
RNGkind
RNGkind
RNGkind(sample.kind = "Rejection")
set.seed(1) # set seed for reproducibility
train_samples = sample(1:nrow(Hitters), round(0.8*nrow(Hitters)))
Hitters_train = Hitters %>% filter(row_number() %in% train_samples)
Hitters_test = Hitters %>% filter(!(row_number() %in% train_samples))
Hitters_train %>% ggplot(aes(x = CAtBat, y = Hits, colour = Salary)) +
geom_point() + theme_bw()
tree_fit = rpart(Salary ~ ., data = Hitters_train)
rpart.plot(tree_fit)
tree_fit
tree_fit$variable.importance
# this code is not meant to be run
control = rpart.control(minsplit = 20, minbucket = round(minsplit/3))
tree_fit$variable.importance
tree_fit = rpart(Salary ~ ., data = Hitters_train)
rpart.plot(tree_fit)
tree_fit
tree_fit$variable.importance
# this code is not meant to be run
control = rpart.control(minsplit = 20, minbucket = round(minsplit/3))
library(rpart)             # install.packages("rpart")
library(rpart.plot)        # install.packages("rpart.plot")
=======
poverty$fips = as.numeric(paste(poverty$`State FIPS Code`, poverty$`County FIPS Code`, sep = ""))
merge1 = ACS_election_land %>% inner_join(poverty, by = "fips") %>%
select(-STATE, -CLASSFP, -Areaname, -`State FIPS Code`, -`County FIPS Code`)
file = "../data/raw/scorecard.csv"
scorecard_raw = read_csv(file)
scorecard_raw = scorecard_raw %>% select("fips_state_code", "fips_county_code",
"calc_police_violence_score","calc_police_accountability_score",
"calc_approach_to_policing_score","calc_police_funding_score")
scorecard_raw = data.frame(sapply(scorecard_raw, function(x) as.numeric(gsub("%", "", x)))) %>%
rename(police_violence_score = calc_police_violence_score,
police_accountability_score=calc_police_accountability_score,
approach_to_policing_score = calc_approach_to_policing_score,
police_funding_score = calc_police_funding_score) %>%
mutate(fips_state_code=as.character(fips_state_code), fips_county_code = as.character(fips_county_code))
scorecard_raw$fips_county_code = str_pad(scorecard_raw$fips_county_code, 3, pad = "0")
scorecard_raw$fips = str_c(scorecard_raw$fips_state_code, scorecard_raw$fips_county_code)
scorecard_clean = scorecard_raw %>% select(-fips_state_code,-fips_county_code) %>%
mutate(fips = as.numeric(fips)) %>% group_by(fips) %>%
summarise(police_violence_score = mean(police_violence_score),
police_accountability_score = mean(police_accountability_score),
approach_to_policing_score = mean(approach_to_policing_score),
police_funding_score = mean(police_funding_score))
merge2 = merge1 %>% inner_join(scorecard_clean, by = "fips")
hha_raw = read_excel('../data/raw/HHS_OCDO_TDWG_COVID-19_Community_Vulnerability_Crosswalk.xlsx') %>% as_tibble()
mean_data = hha_raw %>% select(`County FIPS`,`HHA Score`) %>%
rename(fips = `County FIPS`, hha_score = `HHA Score`) %>%
mutate(fips=as.numeric(fips))%>%
group_by(fips) %>%
summarise(mean_hha_score = mean(hha_score))%>%
as_tibble()
merge3 = merge2 %>% inner_join(mean_data, by = "fips")
file = "../data/raw/Social_Vulnerability_Index_2018.csv"
vulner_raw = read_csv(file)
vulner_raw = vulner_raw %>% select("FIPS","RPL_THEMES")%>% filter(RPL_THEMES>=0)%>%
rename(fips = FIPS, svi_overall=RPL_THEMES)
vulner_raw$fips = as.numeric(vulner_raw$fips)
merge4 = merge3 %>% inner_join(vulner_raw, by = "fips")
state_expenses = read_xlsx("../data/raw/expenditures.xlsx") %>% rename(state = State, spend_per_capita = `State and Local Direct General Expenditures, Per Capita`)
merge6 = merge4 %>% inner_join(state_expenses, by = "state")
unemp_bens = read_xlsx("../data/raw/unemployment bens.xlsx")
stopwords3 = c("Individual", "w/dependents", "up", "to")
unemp_bens$amount = removeWords(unemp_bens$amount, stopwords3)
unemp_bens = unemp_bens %>% separate(amount, into = c("individual", "w_dependents"), sep = "  ") %>% mutate(individual = extract_numeric(individual), w_dependents = extract_numeric(w_dependents))
unemp_bens$w_dependents = ifelse(is.na(unemp_bens$w_dependents), unemp_bens$individual, unemp_bens$w_dependents)
unemp_bens = unemp_bens %>% mutate(average_bens = (as.numeric(individual)+as.numeric(w_dependents))/2) %>% mutate(unemp_bens_possible = average_bens*weeks) %>% select(state, unemp_bens_possible)
merge7 = merge6 %>% inner_join(unemp_bens, by = "state")
census_data_zip <- get_acs(geography = "zcta", variables = c(totalpop = "B01003_001"),
year = 2018)
census_step_zip = census_data_zip %>%
pivot_wider(id_cols = c(NAME), names_from = variable, values_from = estimate) %>%
separate(NAME, into = c("zcta", "zip"), sep = " ") %>% select(-zcta) %>% mutate(zip = as.numeric(zip))
file = "../data/raw/irstiny.csv"
irs_raw = read_csv(file)
irs_raw = irs_raw%>%select(zipcode, N07240)%>% mutate(zip = as.numeric(zipcode),saverscredit = N07240)%>%select(-zipcode,-N07240) %>% group_by(zip) %>% summarise(saverscredit = sum(saverscredit))%>%ungroup()
irs_clean = irs_raw%>% inner_join(census_step_zip, by="zip") %>% mutate(saversperpop=saverscredit/totalpop)
file = "../data/raw/ZIP-COUNTY-FIPS_2017-06.csv"
key_zip_county = read_csv(file)
key_zip_county = key_zip_county %>% rename(zip=ZIP, county=COUNTYNAME, fips=STCOUNTYFP)%>% mutate(zip=as.numeric(zip)) %>% select(zip, county,fips)
irs_joined = key_zip_county%>%inner_join(irs_clean, by="zip")
irs_final = irs_joined%>%group_by(fips)%>% summarise(saversperpop = mean(saversperpop)) %>% ungroup() %>% mutate(fips=as.numeric(fips))
merge8=merge7 %>%inner_join(irs_final,by="fips") %>% drop_na()
write.csv("MBDS/471/final-project-template/data/clean/merge8.csv")
write_csv("MBDS/471/final-project-template/data/clean/merge8.csv")
write.csv2("MBDS/471/final-project-template/data/clean/merge8.csv")
write.csv2("merge8.csv")
write.csv2(merge8, "MBDS/471/final-project-template/data/clean/merge8.csv")
write.csv(merge8, "MBDS/471/final-project-template/data/clean/merge8.csv")
?write.csv
write.csv(merge8, file = "MBDS/471/final-project-template/data/clean/merge8.csv")
write.csv(merge8, file = "MBDS/471/final-project-template/data/clean/merge8.csv")
write.csv(merge8, file = "MBDS/471/final-project-template/data/clean")
write.csv(merge8, file = "MBDS\471\final-project-template\data\clean\\merge8.csv")
write.csv(merge8, file = "MBDS/471/final-project-template/data/clean/merge8.csv")
write.csv(merge8, file = 'MBDS/471/final-project-template/data/clean/merge8.csv')
write.csv(merge8, file = 'merge8.csv')
write.csv(merge8, file = '../data/clean/merge8.csv')
knitr::opts_chunk$set(echo = TRUE)
merge8 = read_csv("../data/clean/merge8.csv")
url = "https://raw.githubusercontent.com/themarshallproject/incarceration-census/main/census_incarceration.csv"
incar_raw = read_csv(url) %>% select(FIPS, state, county, total_population_20, incarcerated_20)%>%
mutate(incar_rate = incarcerated_20/total_population_20) %>%
select(incar_rate, FIPS) %>% rename(fips = FIPS) %>% ungroup()
incarvars = merge8 %>% inner_join(incar_raw, by = "fips") %>% select(-totalpop) %>% drop_na() %>% as.data.frame()
incar_0 = incarvars %>% filter(incar_rate == 0)
incar_complete = incarvars %>% filter(!(incar_rate == 0)) %>% drop_na()
incar_0rf = incar_0 %>% select(-fips, -county, -state)
incar_completerf = incar_complete %>% as.data.frame() %>% select(-fips, -county, -state)
train_samples = sample(1:nrow(incar_completerf), round(0.8*nrow(incar_completerf)))
incar_train = incar_completerf %>% filter(row_number() %in% train_samples)
incar_test = incar_completerf %>% filter(!(row_number() %in% train_samples))
rf_fit = randomForest(incar_rate ~ ., data = incar_train)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(lubridate)
>>>>>>> 177a9b23f1c37511c750e2e50b525ff6c344ef0b
library(tidyverse)
Hitters = ISLR2::Hitters %>%
as_tibble() %>%
filter(!is.na(Salary)) %>%   # remove NA values (in general not necessary)
mutate(Salary = log(Salary)) # log-transform the salary
Hitters
set.seed(1) # set seed for reproducibility
train_samples = sample(1:nrow(Hitters), round(0.8*nrow(Hitters)))
Hitters_train = Hitters %>% filter(row_number() %in% train_samples)
Hitters_test = Hitters %>% filter(!(row_number() %in% train_samples))
Hitters_train %>% ggplot(aes(x = CAtBat, y = Hits, colour = Salary)) +
geom_point() + theme_bw()
tree_fit = rpart(Salary ~ ., data = Hitters_train)
rpart.plot(tree_fit)
tree_fit
tree_fit$variable.importance
# this code is not meant to be run
control = rpart.control(minsplit = 20, minbucket = round(minsplit/3))
tree_fit_2 = rpart(Salary ~ .,
control = rpart.control(minsplit = 80),
data = Hitters_train)
rpart.plot(tree_fit_2)
url = "https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/master/Notebooks/Data/Heart.csv"
Heart = read_csv(url) %>% select(-...1)
Heart
set.seed(1) # set seed for reproducibility
train_samples = sample(1:nrow(Heart), round(0.8*nrow(Heart)))
Heart_train = Heart %>% filter(row_number() %in% train_samples)
Heart_test = Heart %>% filter(!(row_number() %in% train_samples))
tree_fit = rpart(AHD ~ .,
method = "class",              # classification
parms = list(split = "gini"),  # Gini index for splitting
data = Heart_train)
rpart.plot(tree_fit)
pred = predict(tree_fit, newdata = Heart_test)
pred %>% head()
pred = predict(tree_fit, newdata = Heart_test, type = "class")
pred
Hitters = ISLR2::Hitters %>%
as_tibble() %>%
filter(!is.na(Salary)) %>%
mutate(Salary = log(Salary)) # log-transform the salary
library(randomForest)       # install.packages("randomForest")
library(tidyverse)
RNGkind(sample.kind = "Rejection")
Hitters = ISLR2::Hitters %>%
as_tibble() %>%
filter(!is.na(Salary)) %>%
mutate(Salary = log(Salary)) # log-transform the salary
Hitters
set.seed(1) # set seed for reproducibility
train_samples = sample(1:nrow(Hitters), round(0.8*nrow(Hitters)))
Hitters_train = Hitters %>% filter(row_number() %in% train_samples)
Hitters_test = Hitters %>% filter(!(row_number() %in% train_samples))
rf_fit = randomForest(Salary ~ ., data = Hitters_train)
?randomForest
plot(rf_fit)
rf_fit = randomForest(Salary ~ ., mtry = 19, data = Hitters_train)
plot(rf_fit)
rf_3 = randomForest(Salary ~ ., mtry = 3, data = Hitters_train)
rf_6 = randomForest(Salary ~ ., mtry = 6, data = Hitters_train)
rf_19 = randomForest(Salary ~ ., mtry = 19, data = Hitters_train)
#MSE = OOB error
oob_errors = bind_rows(
tibble(ntree = 1:500, oob_err = rf_3$mse, m = 3),
tibble(ntree = 1:500, oob_err = rf_6$mse, m = 6),
tibble(ntree = 1:500, oob_err = rf_19$mse, m = 19)
)
oob_errors
oob_errors %>%
ggplot(aes(x = ntree, y = oob_err, colour = factor(m))) +
geom_line() + theme_bw()
# might want to cache this chunk!
mvalues = seq(1,19, by = 2)
oob_errors = numeric(length(mvalues))
ntree = 500
for(idx in 1:length(mvalues)){
m = mvalues[idx]
rf_fit = randomForest(Salary ~ ., mtry = m, data = Hitters_train)
oob_errors[idx] = rf_fit$mse[ntree]
}
tibble(m = mvalues, oob_err = oob_errors) %>%
ggplot(aes(x = m, y = oob_err)) +
geom_line() + geom_point() +
scale_x_continuous(breaks = mvalues) +
theme_bw()
rf_fit = randomForest(Salary ~ ., data = Hitters_train)
rf_fit$importance
varImpPlot(rf_fit)
rf_fit = randomForest(Salary ~ ., importance = TRUE, data = Hitters_train)
rf_fit$importance
varImpPlot(rf_fit)
rf_predictions = predict(rf_fit, newdata = Hitters_test)
rf_predictions
mean((rf_predictions - Hitters_test$Salary)^2)
knitr::opts_chunk$set(echo = TRUE)
#load the rda file
load(file = "37323-0001-Data.rda")
#load the rda file
load(file = "37323-0001-Data.rda")
knitr::opts_chunk$set(echo = TRUE)
#load the rda file
load(file = "37323-0001-Data.rda")
#load the rda file
load(file = "37323-0001-Data.rda")
#load the rda file
load(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
View(da37323.0001)
lemas <- readRDS(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
#load the rda file
lemas=load(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
file.exists("/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
filename <- file.choose("37323-0001-Data")
lemas <- readRDS(filename)
filename <- file.choose("37323-0001-Data")
lemas <- readRDS(filename)
#load the rda file
load(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
#load the rda file
load(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/final-project/lemas2016/37323-0001-Data.rda")
View(da37323.0001)
url = "https://raw.githubusercontent.com/JieYingWu/COVID-19_US_County-level_Summaries/master/data/counties.csv"
jieyingwu = read_csv(url)
library(tidyverse)
library(tidyverse)
url = "https://raw.githubusercontent.com/JieYingWu/COVID-19_US_County-level_Summaries/master/data/counties.csv"
jieyingwu = read_csv(url)
jieyingwu
library(tidyverse)
url = "https://raw.githubusercontent.com/JieYingWu/COVID-19_US_County-level_Summaries/master/data/counties.csv"
jieyingwu = read_csv(url)
View(jieyingwu)
load(file ="/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/final-project/jailsurvey/37392-0001-Data.rda")
knitr::opts_chunk$set(echo = TRUE)
load(file ="/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/final-project/jailsurvey/37392-0001-Data.rda")
View(da37392.0001)
jieyingwu
view(jieyingwu)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(readr)
library(tidycensus)
library(rpart)
library(rpart.plot)
library(ipred)
library(readxl)
library(usdata)
library(tm)
library(stringr)
var <- load_variables(2019, "acs1", cache = TRUE)
census_api_key("75692c32a90eb5b381e167037e4341b90af4c5d3", overwrite = TRUE, install = TRUE)
census_data <- get_acs(geography = "county", variables = c(med_age = "B01002_001",
permale = "B01001_002",
bachplus = "B16010_041",
totalpop = "B01003_001",
unemployed = "B27011_008",
employed =	"B27011_003",
foodstamp = "B09010_002",
ilefnhi= "B27011_007",
ilufnhi= "B27011_012",
nilfnhi = "B27011_017",
gini = "B19083_001",
med_income = "B19013_001",
med_2bed = "B25031_004",
single_mom = "B11012_010",
lessthan_hs = "B16010_002",
housing_units = "B25001_001"),
year = 2019)
options(scipen = 0, digits = 3)  # controls number of significant digits printed
library(rpart)         # to train decision trees
library(rpart.plot)    # to plot decision trees
library(randomForest)  # random forests
library(gbm)           # boosting
library(tidyverse)     # tidyverse
library(kableExtra)    # for printing tables
library(cowplot)       # for side by side plots
theftdata=read_csv("../data/clean/dataclean.csv")
set.seed(471) # set seed for reproducibility
train_samples = sample(1:nrow(theftdata), round(0.8*nrow(theftdata)))
theft_train = theftdata %>% filter(row_number() %in% train_samples) %>% select(-county,-state)
theft_test = theftdata %>% filter(!(row_number() %in% train_samples))%>% select(-county,-state)
set.seed(471) # set seed for reproducibility
mvalues = seq(3,15, by = 1)
oob_errors = numeric(length(mvalues))
ntree = 500
for(idx in 1:length(mvalues)){
m = mvalues[idx]
rf_fit = randomForest(theftrate ~ ., mtry = m, data = theft_train)
oob_errors[idx] = rf_fit$mse[ntree]
}
tibble(m = mvalues, oob_err = oob_errors) %>%
ggplot(aes(x = m, y = oob_err)) +
geom_line() + geom_point() +
scale_x_continuous(breaks = mvalues) +
theme_bw()
set.seed(471) # set seed for reproducibility
rf_6 = randomForest(theftrate ~ ., mtry = 6, data = theft_train)
rf_6$importance
varImpPlot(rf_6,n.var = 10)
library(tidyverse)
library(glmnetUtils)
source("/code/functions/plot_glmnet.R")
source("../code/functions/plot_glmnet.R")
View(theftdata)
options(scipen = 0, digits = 3)  # controls number of significant digits printed
library(rpart)         # to train decision trees
library(rpart.plot)    # to plot decision trees
library(randomForest)  # random forests
library(gbm)           # boosting
library(tidyverse)     # tidyverse
library(kableExtra)    # for printing tables
library(cowplot)       # for side by side plots
theftdata=read_csv("../data/clean/dataclean.csv")
set.seed(471) # set seed for reproducibility
train_samples = sample(1:nrow(theftdata), round(0.8*nrow(theftdata)))
theft_train = theftdata %>% filter(row_number() %in% train_samples) %>% select(-county,-state)
theft_test = theftdata %>% filter(!(row_number() %in% train_samples))%>% select(-county,-state)
set.seed(471) # set seed for reproducibility
mvalues = seq(3,15, by = 1)
oob_errors = numeric(length(mvalues))
ntree = 500
for(idx in 1:length(mvalues)){
m = mvalues[idx]
rf_fit = randomForest(theftrate ~ -fips ., mtry = m, data = theft_train)
options(scipen = 0, digits = 3)  # controls number of significant digits printed
library(rpart)         # to train decision trees
library(rpart.plot)    # to plot decision trees
library(randomForest)  # random forests
library(gbm)           # boosting
library(tidyverse)     # tidyverse
library(kableExtra)    # for printing tables
library(cowplot)       # for side by side plots
theftdata=read_csv("../data/clean/dataclean.csv")
set.seed(471) # set seed for reproducibility
train_samples = sample(1:nrow(theftdata), round(0.8*nrow(theftdata)))
theft_train = theftdata %>% filter(row_number() %in% train_samples) %>% select(-county,-state)
theft_test = theftdata %>% filter(!(row_number() %in% train_samples))%>% select(-county,-state)
set.seed(471) # set seed for reproducibility
mvalues = seq(3,15, by = 1)
oob_errors = numeric(length(mvalues))
ntree = 500
for(idx in 1:length(mvalues)){
m = mvalues[idx]
rf_fit = randomForest(theftrate ~. -fips, mtry = m, data = theft_train)
oob_errors[idx] = rf_fit$mse[ntree]
}
tibble(m = mvalues, oob_err = oob_errors) %>%
ggplot(aes(x = m, y = oob_err)) +
geom_line() + geom_point() +
scale_x_continuous(breaks = mvalues) +
theme_bw()
set.seed(471) # set seed for reproducibility
rf_6 = randomForest(theftrate ~ .-fips, mtry = 6, data = theft_train)
rf_6$importance
varImpPlot(rf_6,n.var = 10)
set.seed(471) # set seed for reproducibility
rf_6 = randomForest(theftrate ~ .-fips, mtry = 4, data = theft_train)
rf_6$importance
varImpPlot(rf_6,n.var = 10)
library(tidyverse)
library(glmnetUtils)
source("../code/functions/plot_glmnet.R")
theftdata=read_csv("../data/clean/dataclean.csv")
set.seed(471) # set seed for reproducibility
train_samples = sample(1:nrow(theftdata), round(0.8*nrow(theftdata)))
theft_train = theftdata %>% filter(row_number() %in% train_samples) %>% select(-county,-state)
theft_test = theftdata %>% filter(!(row_number() %in% train_samples))%>% select(-county,-state)
set.seed(471) # set seed for reproducibility
ridge_fit = cv.glmnet(theftrate ~ .-fips,  # formula notation, as usual
alpha = 0,                 # alpha = 0 for ridge
nfolds = 10,               # number of folds
data = theft_train)   # data to run ridge on
plot(ridge_fit)
plot_glmnet(ridge_fit, crime_data_train, features_to_plot = 10)
plot_glmnet(ridge_fit, theft_train, features_to_plot = 10)
plot_glmnet(ridge_fit, theft_train, features_to_plot = 7)
plot_glmnet(ridge_fit, theft_train, features_to_plot = 8)
setwd("/Users/diyangchu/Documents/2-grad@Penn/STAT471/theft-crime-2020")
library(corrplot)
theft_train=read_csv("../data/clean/theft_train.csv")
theft_train=read.csv("../data/clean/theft_train.csv")
theft_train = theft_train%>% select(-fips, -state, -county)
library(corrplot)
library(tidyverse)
theft_train=read.csv("../data/clean/theft_train.csv")
theft_train = theft_train%>% select(-fips, -state, -county)
M = cor(theft_train)
corrplot(M, method = 'square', diag = FALSE, order = 'hclust',tl.cex = 0.4,
addrect = 3, rect.col = 'blue', rect.lwd = 3, tl.pos = 'd')
corrplot(M, type = 'lower', order = 'hclust', tl.col = 'black',
cl.ratio = 0.2, tl.srt = 45, col = COL2('PuOr', 10), tl.cex = 0.4)
theft_train=read.csv("../data/clean/theft_train.csv")
theft_train = theft_train%>% select(-fips, -state, -county)
M = cor(theft_train)
corrplot(M, method = 'square', diag = FALSE, order = 'hclust',tl.cex = 0.4,
addrect = 3, rect.col = 'blue', rect.lwd = 3, tl.pos = 'd')
theft_train=read_csv("../data/clean/theft_train.csv")
theft_train = theft_train%>% select(-fips, -state, -county)
M = cor(theft_train)
corrplot(M, method = 'square', diag = FALSE, order = 'hclust',tl.cex = 0.4,
addrect = 3, rect.col = 'blue', rect.lwd = 3, tl.pos = 'd')
corrplot(M, method = 'square', diag = FALSE, order = 'hclust',tl.cex = 0.4,
addrect = 3, rect.col = 'blue', rect.lwd = 3, tl.pos = 'd')
corrplot(M, type = 'lower', order = 'hclust', tl.col = 'black',
cl.ratio = 0.2, tl.srt = 45, col = COL2('PuOr', 10), tl.cex = 0.4)
is.na(theft_train) %>% table()
is.infinite(theft_train) %>% table()
theft_train[sapply(theft_train, is.infinite)] <- 0
is.infinite(theft_train)
View(theft_train)
M = cor(theft_train)
corrplot(M, method = 'square', diag = FALSE, order = 'hclust',tl.cex = 0.4,
addrect = 3, rect.col = 'blue', rect.lwd = 3, tl.pos = 'd')
corrplot(M, type = 'lower', order = 'hclust', tl.col = 'black',
cl.ratio = 0.2, tl.srt = 45, col = COL2('PuOr', 10), tl.cex = 0.4)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(readr)
library(readxl)
library(usdata)
library(tm)
library(stringr)
library(kableExtra)
theft_train=read_csv("/Users/diyangchu/Documents/2-grad@Penn/STAT471/theft-crime-2020/data/clean/theft_train.csv")
theft_train[sapply(theft_train, is.infinite)] <- 0
ggplot(aes(x = theftrate)) +
geom_histogram()+
labs(y = "Count",
x = "Theft Rate",
title = "Histogram of Theft Rate")+
geom_vline(aes(xintercept = mean(time)),
colour = "red", linetype ="longdash", size = .8)+
theme_bw()+
scale_y_log10()
theft_train %>% ggplot(aes(x = theftrate)) +
geom_histogram()+
labs(y = "Count",
x = "Theft Rate",
title = "Histogram of Theft Rate")+
geom_vline(aes(xintercept = mean(time)),
colour = "red", linetype ="longdash", size = .8)+
theme_bw()+
scale_y_log10()
theft_train %>% ggplot(aes(x = theftrate)) +
geom_histogram()+
labs(y = "Count",
x = "Theft Rate",
title = "Histogram of Theft Rate")+
geom_vline(aes(xintercept = mean(theftrate)),
colour = "red", linetype ="longdash", size = .8)+
theme_bw()+
scale_y_log10()
theft_train %>% ggplot(aes(x = theftrate)) +
geom_histogram()+
labs(y = "Count",
x = "Theft Rate",
title = "Histogram of Theft Rate")+
geom_vline(aes(xintercept = mean(theftrate)),
colour = "red", linetype ="longdash", size = .8)+
geom_vline(aes(xintercept = median(theftrate)),
colour = "blue", linetype ="longdash", size = .8)+
theme_bw()+
scale_y_log10()
theft_train %>% ggplot(aes(x = theftrate)) +
geom_histogram()+
labs(y = "Count",
x = "Theft Rate",
title = "Histogram of Theft Rate")+
geom_vline(aes(xintercept = mean(theftrate)),
colour = "red", linetype ="longdash", size = .8)+
geom_vline(aes(xintercept = median(theftrate)),
colour = "blue", linetype ="longdash", size = .8)+
theme_bw()+
scale_y_log10()+
scale_color_manual(name = "statistics", values = c(median = "blue", mean = "red"))
theft_train %>% ggplot(aes(x = theftrate)) +
geom_histogram()+
labs(y = "Count",
x = "Theft Rate",
title = "Histogram of Theft Rate")+
geom_vline(aes(xintercept = mean(theftrate),colour = "red"),
linetype ="longdash", size = .8)+
geom_vline(aes(xintercept = median(theftrate),colour = "blue"),
linetype ="longdash", size = .8)+
theme_bw()+
scale_y_log10()+
scale_color_manual(name = "statistics", values = c(median = "blue", mean = "red"))
theft_train %>% ggplot(aes(x = theftrate)) +
geom_histogram()+
labs(y = "Count",
x = "Theft Rate",
title = "Histogram of Theft Rate")+
geom_vline(aes(xintercept = mean(theftrate),colour = "mean"),
linetype ="longdash", size = .8)+
geom_vline(aes(xintercept = median(theftrate),colour = "median"),
linetype ="longdash", size = .8)+
theme_bw()+
scale_y_log10()+
scale_color_manual(name = "statistics", values = c(median = "blue", mean = "red"))
cluster_safetynet = theft_train %>% select(unemp_bens_possible, spend_per_capita, saversperhouses, no_health_ins, foodstamp)
M = cor(cluster_safetynet)
corrplot(M, type = 'lower', order = 'hclust', tl.col = 'black',
cl.ratio = 0.2, tl.srt = 45, col = COL2('PuOr', 10), tl.cex = 0.4)
corrplot(M, type = 'lower', order = 'hclust', tl.col = 'black',
cl.ratio = 0.2, tl.srt = 45, col = COL2('PuOr', 10), tl.cex = 1)
cluster_criminaljustice = theft_train %>% select(incar_rate, police_violence_score, police_accountability_score, approach_to_policing_score, police_funding_score)
M_criminaljustice = cor(cluster_criminaljustice)
corrplot(M_criminaljustice, type = 'lower', order = 'hclust', tl.col = 'black',
cl.ratio = 0.2, tl.srt = 45, col = COL2('PuOr', 10), tl.cex = 1)
corrplot(M_criminaljustice, type = 'lower', order = 'hclust', tl.col = 'black',
cl.ratio = 0.2, tl.srt = 45, col = COL2('PuOr', 10), tl.cex = 0.8)
cluster_health = theft_train %>% select(mean_hha_score, poor_fair_health, dis5to17, dis18to34, dis35to64)
M_health = cor(cluster_health)
corrplot(M_health, type = 'lower', order = 'hclust', tl.col = 'black',
cl.ratio = 0.2, tl.srt = 45, col = COL2('PuOr', 10), tl.cex = 0.8)
cluster_ses = theft_train %>% select(lessthan_hs, bachplus, unemployed_rate, employed_rate, med_2bed, gini, svi_overall, pct_all_in_pov, pct_child_in_pov, med_income, sev_hou_cost_burden, sev_hou_prob, PctEmpChange1920, PctEmpConstruction, PctEmpMining, PctEmpTrade, PctEmpTrans, PctEmpInformation, PctEmpFIRE, PerCapitaInc, Deep_Pov_All, Deep_Pov_Children, inschool, ingradprofesh, inundergrad)
setwd("/Users/diyangchu/Documents/2-grad@Penn/STAT471/theft-crime-2020/code")
source("1-cleaning.R")
source("2.1-imputation.R")
source("2.2-cleaning2.R")
source("2.3-train-test-split.R")
theft_train=read_csv("../data/clean/theft_train.csv")
setwd("/Users/diyangchu/Documents/2-grad@Penn/STAT471/theft-crime-2020")
theft_train=read_csv("../data/clean/theft_train.csv")
theft_train=read_csv("/Users/diyangchu/Documents/2-grad@Penn/STAT471/theft-crime-2020/data/clean/theft_train.csv")
theft_train = theft_train%>% select(-fips, -state, -county)
View(theft_train)
#setwd("C:/Users/Mason Shihab/Documents/MBDS/471/final-project-template/code")
setwd("/Users/diyangchu/Documents/2-grad@Penn/STAT471/theft-crime-2020/code")
source("1-cleaning.R")
source("2.1-imputation.R")
source("2.2-cleaning2.R")
source("2.3-train-test-split.R")
View(merge11)
atlas_income_raw = read_csv(file)
setwd("/Users/diyangchu/Documents/2-grad@Penn/STAT471/theft-crime-2020/code")
source("1-cleaning.R")
source("2.1-imputation.R")
source("2.2-cleaning2.R")
source("2.3-train-test-split.R")
<<<<<<< HEAD
View(incar_0)
View(theft_train)
View(theft_test)
library(corrplot)
library(tidyverse)
theft_train=read_csv("/Users/diyangchu/Documents/2-grad@Penn/STAT471/theft-crime-2020/data/clean/theft_train.csv")
theft_train = theft_train%>% select(-fips, -state, -county)
cluster_ses = theft_train %>% select(lessthan_hs, bachplus, unemployed_rate, employed_rate, med_2bed, gini, svi_overall, pct_all_in_pov, pct_child_in_pov, med_income, sev_hou_cost_burden, sev_hou_prob, PctEmpChange1920, PctEmpConstruction, PctEmpMining, PctEmpTrade, PctEmpTrans, PctEmpInformation, PctEmpFIRE, PerCapitaInc, Deep_Pov_All, Deep_Pov_Children, inschool, ingradprofesh, inundergrad)
M_ses = cor(cluster_ses)
corrplot(M_ses, type = 'lower', order = 'hclust', tl.col = 'black',
cl.ratio = 0.2, tl.srt = 45, col = COL2('PuOr', 10), tl.cex = 0.8)
cluster_demo= theft_train %>% select(med_age,permale,divorced,widowed,nevermarried,PerTrump,pop_density,housing_density,res_seg_nonwhite_white,Marriedcouplefamily,ForeignBornEuropePct,ForeignBornMexPct,ForeignBornCaribPct,ForeignBornCentralSouthAmPct,ForeignBornAsiaPct,ForeignBornAfricaPct,NonEnglishHHNum,AvgHHSize,PopChangeRate1819,withkids,single_mom,singledad,foreignborn,fromdifstate,fromabroad)
cluster_demo= theft_train %>% select(med_age,permale,divorced,widowed,nevermarried,pertrump,pop_density,housing_density,res_seg_nonwhite_white,Marriedcouplefamily,ForeignBornEuropePct,ForeignBornMexPct,ForeignBornCaribPct,ForeignBornCentralSouthAmPct,ForeignBornAsiaPct,ForeignBornAfricaPct,NonEnglishHHNum,AvgHHSize,PopChangeRate1819,withkids,single_mom,singledad,foreignborn,fromdifstate,fromabroad)
M_demo = cor(cluster_demo)
corrplot(cluster_demo, type = 'lower', order = 'hclust', tl.col = 'black',
cl.ratio = 0.2, tl.srt = 45, col = COL2('PuOr', 10), tl.cex = 0.8)
cluster_demo= theft_train %>% select(med_age,permale,divorced,widowed,nevermarried,pertrump,pop_density,housing_density,res_seg_nonwhite_white,Marriedcouplefamily,ForeignBornEuropePct,ForeignBornMexPct,ForeignBornCaribPct,ForeignBornCentralSouthAmPct,ForeignBornAsiaPct,ForeignBornAfricaPct,NonEnglishHHNum,AvgHHSize,PopChangeRate1819,withkids,single_mom,singledad,foreignborn,fromdifstate,fromabroad)
M_demo = cor(cluster_demo)
corrplot(cluster_demo, type = 'lower', order = 'hclust', tl.col = 'black',
cl.ratio = 0.2, tl.srt = 45, col = COL2('PuOr', 10), tl.cex = 0.8)
View(theft_train)
View(cluster_demo)
View(M_demo)
corrplot(M_demo, type = 'lower', order = 'hclust', tl.col = 'black',
cl.ratio = 0.2, tl.srt = 45, col = COL2('PuOr', 10), tl.cex = 0.8)
=======
source("2-exploration.R")
#source("4-regression-modeling.R")
#source("5-tree-modeling.R")
#source("6-model-evaluation.R")
# run all steps of the analysis pipeline
setwd("C:/Users/Mason Shihab/Documents/MBDS/471/final-project-template/code")
source("1-cleaning.R")
source("2.1-imputation.R")
source("2.2-cleaning2.R")
source("2.3-train-test-split.R")
source("2-exploration.R")
#source("4-regression-modeling.R")
#source("5-tree-modeling.R")
#source("6-model-evaluation.R")
>>>>>>> 177a9b23f1c37511c750e2e50b525ff6c344ef0b
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(readr)
library(readxl)
library(usdata)
library(tm)
library(stringr)
<<<<<<< HEAD
library(kableExtra)
```{r}
theft_train=read_csv("../data/clean/theft_train.csv")
theft_train = theft_train%>% select(-fips, -state, -county,-theftrate)
=======
library(gbm)
library(glmnetUtils) # boosting
library(randomForest)
gbm_fit_optimal = gbm_3
optimal_num_trees = gbm.perf(gbm_1, plot.it = FALSE)
summary(gbm_3, n.trees = optimal_num_trees, plotit = FALSE) %>% tibble() %>%
head(10)
ntrees = 1000
cv_errors = bind_rows(
tibble(ntree = 1:ntrees, cv_err = gbm_1$cv.error, Depth = 1),
tibble(ntree = 1:ntrees, cv_err = gbm_2$cv.error, Depth = 2),
tibble(ntree = 1:ntrees, cv_err = gbm_3$cv.error, Depth = 3)
) %>% mutate(Depth = factor(Depth))
# plot CV errors
mins = cv_errors %>% group_by(Depth) %>% summarise(min_err = min(cv_err))
gbm.perf(gbm_3, plot.it = FALSE)
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = Depth)) +
geom_line() + theme_bw() +
geom_hline(aes(yintercept = min_err, color = Depth),
data = mins, linetype = "dashed") +
labs(y = "CV Error", x = "Trees") + scale_y_log10()
>>>>>>> 177a9b23f1c37511c750e2e50b525ff6c344ef0b
