<<<<<<< HEAD
colour = "red", linetype ="longdash", size = .8)+
labs(x = "Acceptance Rate", y = "Count") +
theme_bw()
median(college_train$Accept)
college_train %>% arrange(Accept) %>%           # arrange in increasing order
head(1) %>% select(Name,Accept)
college_train%>%ggplot(aes(x=Grad.Rate, y=Accept)) +
geom_point()+
labs(y = "Acceptance Rate", x = "Graduation Rate") +
theme_bw()
college_train%>%ggplot(aes(x=Top10perc, y=Accept)) +
geom_point()+
labs(y = "Acceptance Rate", x = "Percentage of Top 10% Students") +
theme_bw()
college_train%>%ggplot(aes(x=Room.Board, y=Accept)) +
geom_point()+
labs(y = "Acceptance Rate", x = "Room & Board Costs") +
theme_bw()
college_train %>%
filter(Name=="Harvard University")%>%
select(Top10perc)
college_train %>%
arrange(desc(Top10perc))%>%
head(1)%>%
select(Name, Accept, Top10perc)
college_train = college_train %>% select(-Name)
college_test = college_test %>% select(-Name)
lm_fit=lm(Accept~., data = college_train)
summary(lm_fit)
set.seed(3) # set seed before cross-validation for reproducibility
ridge_fit = cv.glmnet(Accept ~ .,  # formula notation, as usual
alpha = 0,                 # alpha = 0 for ridge
nfolds = 10,               # number of folds
data = college_train)   # data to run ridge on
#get CV plot
plot(ridge_fit)
# lambda based on one-standard-error rule
ridge_fit$lambda.1se
set.seed(3) # set seed before cross-validation for reproducibility
ridge_fit = cv.glmnet(Accept ~ .,  # formula notation, as usual
alpha = 0,                 # alpha = 0 for ridge
nfolds = 10,               # number of folds
data = college_train)   # data to run ridge on
#get CV plot
plot(ridge_fit)
# lambda based on one-standard-error rule
ridge_fit$lambda.1se
length(ridge_fit$lambda)
plot_glmnet(ridge_fit, college_train, features_to_plot = 6)
coeffs = tibble(lm_coef = coef(lm_fit)[-1],
ridge_coef = coef(ridge_fit, s = "lambda.1se")[-1,1],
features = names(coef(lm_fit)[-1]))
coeffs
coeffs %>%
filter((lm_coef>0 & ridge_coef<0)|(lm_coef<0 & ridge_coef>0))%>%
summarise(num_diff_sign=n())
coeffs %>%
filter(abs(lm_coef)<abs(ridge_coef))%>%
summarise(num_smaller=n())
set.seed(5) # set seed before cross-validation for reproducibility
lasso_fit = cv.glmnet(Accept ~ .,
alpha = 1,                 # alpha = 1 for lasso
nfolds = 10,               # number of folds
data = college_train)   # data to run lasso on
#get the CV plot
plot(lasso_fit)
plot_glmnet(lasso_fit, college_train)
install.packages(c("rmarkdown", "tidyr", "tinytex"))
install.packages(c("stringi", "systemfonts", "tibble", "vroom", "xfun"))
install.packages(c("stringi", "systemfonts", "tibble", "vroom", "xfun"))
install.packages(c("stringi", "systemfonts", "tibble", "vroom", "xfun"))
install.packages(c("cpp11", "data.table", "digest", "hms", "ISLR2", "knitr"))
install.packages(c("openssl", "pillar", "readr", "rlang", "rvest"))
install.packages(c("lattice", "lifecycle", "lubridate", "mgcv", "mime", "nlme"))
options(scipen = 0, digits = 3)  # controls number of significant digits printed
exp(0.0166)
set.seed(1)
sample(1:10,1)
RNGkind()
RNGkind(sample.kind = "Rejection")
RNGkind()
set.seed(1)
sample(1:10,1)
options(scipen = 0, digits = 3)  # controls number of significant digits printed
library(kableExtra) # for printing tables
library(cowplot)    # for side by side plots
library(glmnet)     # to run ridge and lasso
library(ISLR2)      # necessary for College data
library(pROC)       # for ROC curves
library(tidyverse)
library(glmnetUtils)
# install.packages("scales")              # dependency of plot_glmnet
source("../../functions/plot_glmnet.R")
hd_data = read_csv("../../data/Framingham.dat",col_names = TRUE,col_types = "iifiiiii")
hd_data=as_tibble(hd_data)
hd_data = hd_data %>% rename(c('HD'='Heart Disease?')) %>% na.omit()
hd_data %>%
summarise(mean(HD))
set.seed(5) # seed set for reproducibility (DO NOT CHANGE)
n = nrow(hd_data)
train_samples = sample(1:n, round(0.8*n))
hd_train = hd_data %>% filter(row_number() %in% train_samples)
hd_test = hd_data %>% filter(!(row_number() %in% train_samples))
ggplot(hd_train, aes(x = AGE)) +
geom_histogram(binwidth =1, colour = "blue", fill = "white")+
geom_vline(aes(xintercept = median(AGE)),
colour = "red", linetype ="longdash", size = .8)+
labs(y = "Count", x = "Age") +
theme_bw()
ggplot(hd_train) +
geom_boxplot(aes(x=as.factor(HD), y=SBP, fill=as.factor(HD)))+
labs(y = "SBP (systolic blood pressure)", x = "Heart Disease Condition") +
theme_bw()
hd_train_subset = hd_train %>%
filter(CIG ==40,CHOL >=260) %>% select(HD,SBP)
hd_train_subset
glm_sub_fit = glm(HD ~ SBP,
family = "binomial",
data = hd_train_subset)
coef(glm_sub_fit)
b0=-10.1427
b1=0.0737
exp(b0+b1*150)/(1+exp(b0+b1*150))
exp(b0+b1*130)/(1+exp(b0+b1*130))
exp(b0+b1*190)/(1+exp(b0+b1*190))
exp(b0+b1*142)/(1+exp(b0+b1*142))
likelihood = function(beta_1)((exp((-10.1427)+beta_1*150)/(1+exp((-10.1427)+beta_1*150)))*
(exp((-10.1427)+beta_1*130)/(1+exp((-10.1427)+beta_1*130)))*
(exp((-10.1427)+beta_1*190)/(1+exp((-10.1427)+beta_1*190)))*
(1/(1+exp((-10.1427)+beta_1*142)))*(1/(1+exp((-10.1427)+beta_1*130))))
ggplot(data.frame(x = c(0, 0.1)),aes(x = x)) +
stat_function(fun = likelihood)+
geom_vline(aes(xintercept = 0.0737),
colour = "red", linetype ="longdash", size = .8)+
labs(x = "beta_1", y = "Likelihood") +
theme_bw()
glm_fit = glm(HD ~ SBP,
family = "binomial",
data = hd_train)
coef(glm_fit)
exp(0.0166)
hd_train %>%
ggplot(aes(x = SBP, y = HD))+
geom_jitter(height = .05) +
geom_smooth(method = "glm",
formula = "y~x",
method.args = list(family = "binomial"),
se = FALSE) +
ylab("Prob(HD=1)") +
theme_bw()
glm_multi_fit = glm(HD~.,
family = "binomial",
data = hd_train)
coef(glm_multi_fit)
exp(0.06151*10)
logodds_mary=coef(glm_multi_fit)[1]+coef(glm_multi_fit)[2]*50+
coef(glm_multi_fit)[3]*1+coef(glm_multi_fit)[4]*110+
coef(glm_multi_fit)[5]*80+coef(glm_multi_fit)[6]*180+
coef(glm_multi_fit)[7]*105+coef(glm_multi_fit)[8]*0
prob_mary = exp(logodds_mary)/(1+exp(logodds_mary))
prob_mary
coef(glm_multi_fit) %>% pull()
coef(glm_multi_fit) %>% as.numeric()
logodds_mary=coef(glm_multi_fit)[1]+coef(glm_multi_fit)[2]*50+
coef(glm_multi_fit)[3]*1+coef(glm_multi_fit)[4]*110+
coef(glm_multi_fit)[5]*80+coef(glm_multi_fit)[6]*180+
coef(glm_multi_fit)[7]*105+coef(glm_multi_fit)[8]*0
prob_mary = exp(logodds_mary)/(1+exp(logodds_mary))
prob_mary
coef_vec= coef(glm_multi_fit) %>% as.numeric()
coef_vec[1]
logodds_mary=coef_vec[1]+coef_vec[2]*50+
coef_vec[3]*1+coef_vec[4]*110+
coef_vec[5]*80+coef_vec[6]*180+
coef_vec[7]*105+coef_vec[8]*0
prob_mary = exp(logodds_mary)/(1+exp(logodds_mary))
prob_mary
library(rpart)             # install.packages("rpart")
library(rpart.plot)        # install.packages("rpart.plot")
library(tidyverse)
Hitters = ISLR2::Hitters %>%
as_tibble() %>%
filter(!is.na(Salary)) %>%   # remove NA values (in general not necessary)
mutate(Salary = log(Salary)) # log-transform the salary
Hitters
RNGkind
RNGkind
RNGkind(sample.kind = "Rejection")
set.seed(1) # set seed for reproducibility
train_samples = sample(1:nrow(Hitters), round(0.8*nrow(Hitters)))
Hitters_train = Hitters %>% filter(row_number() %in% train_samples)
Hitters_test = Hitters %>% filter(!(row_number() %in% train_samples))
Hitters_train %>% ggplot(aes(x = CAtBat, y = Hits, colour = Salary)) +
geom_point() + theme_bw()
tree_fit = rpart(Salary ~ ., data = Hitters_train)
rpart.plot(tree_fit)
tree_fit
tree_fit$variable.importance
# this code is not meant to be run
control = rpart.control(minsplit = 20, minbucket = round(minsplit/3))
tree_fit$variable.importance
tree_fit = rpart(Salary ~ ., data = Hitters_train)
rpart.plot(tree_fit)
tree_fit
tree_fit$variable.importance
# this code is not meant to be run
control = rpart.control(minsplit = 20, minbucket = round(minsplit/3))
library(rpart)             # install.packages("rpart")
library(rpart.plot)        # install.packages("rpart.plot")
library(tidyverse)
Hitters = ISLR2::Hitters %>%
as_tibble() %>%
filter(!is.na(Salary)) %>%   # remove NA values (in general not necessary)
mutate(Salary = log(Salary)) # log-transform the salary
Hitters
set.seed(1) # set seed for reproducibility
train_samples = sample(1:nrow(Hitters), round(0.8*nrow(Hitters)))
Hitters_train = Hitters %>% filter(row_number() %in% train_samples)
Hitters_test = Hitters %>% filter(!(row_number() %in% train_samples))
Hitters_train %>% ggplot(aes(x = CAtBat, y = Hits, colour = Salary)) +
geom_point() + theme_bw()
tree_fit = rpart(Salary ~ ., data = Hitters_train)
rpart.plot(tree_fit)
tree_fit
tree_fit$variable.importance
# this code is not meant to be run
control = rpart.control(minsplit = 20, minbucket = round(minsplit/3))
tree_fit_2 = rpart(Salary ~ .,
control = rpart.control(minsplit = 80),
data = Hitters_train)
rpart.plot(tree_fit_2)
url = "https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/master/Notebooks/Data/Heart.csv"
Heart = read_csv(url) %>% select(-...1)
Heart
set.seed(1) # set seed for reproducibility
train_samples = sample(1:nrow(Heart), round(0.8*nrow(Heart)))
Heart_train = Heart %>% filter(row_number() %in% train_samples)
Heart_test = Heart %>% filter(!(row_number() %in% train_samples))
tree_fit = rpart(AHD ~ .,
method = "class",              # classification
parms = list(split = "gini"),  # Gini index for splitting
data = Heart_train)
rpart.plot(tree_fit)
pred = predict(tree_fit, newdata = Heart_test)
pred %>% head()
pred = predict(tree_fit, newdata = Heart_test, type = "class")
pred
Hitters = ISLR2::Hitters %>%
as_tibble() %>%
filter(!is.na(Salary)) %>%
mutate(Salary = log(Salary)) # log-transform the salary
library(randomForest)       # install.packages("randomForest")
library(tidyverse)
RNGkind(sample.kind = "Rejection")
Hitters = ISLR2::Hitters %>%
as_tibble() %>%
filter(!is.na(Salary)) %>%
mutate(Salary = log(Salary)) # log-transform the salary
Hitters
set.seed(1) # set seed for reproducibility
train_samples = sample(1:nrow(Hitters), round(0.8*nrow(Hitters)))
Hitters_train = Hitters %>% filter(row_number() %in% train_samples)
Hitters_test = Hitters %>% filter(!(row_number() %in% train_samples))
rf_fit = randomForest(Salary ~ ., data = Hitters_train)
?randomForest
plot(rf_fit)
rf_fit = randomForest(Salary ~ ., mtry = 19, data = Hitters_train)
plot(rf_fit)
rf_3 = randomForest(Salary ~ ., mtry = 3, data = Hitters_train)
rf_6 = randomForest(Salary ~ ., mtry = 6, data = Hitters_train)
rf_19 = randomForest(Salary ~ ., mtry = 19, data = Hitters_train)
#MSE = OOB error
oob_errors = bind_rows(
tibble(ntree = 1:500, oob_err = rf_3$mse, m = 3),
tibble(ntree = 1:500, oob_err = rf_6$mse, m = 6),
tibble(ntree = 1:500, oob_err = rf_19$mse, m = 19)
)
oob_errors
oob_errors %>%
ggplot(aes(x = ntree, y = oob_err, colour = factor(m))) +
geom_line() + theme_bw()
# might want to cache this chunk!
mvalues = seq(1,19, by = 2)
oob_errors = numeric(length(mvalues))
ntree = 500
for(idx in 1:length(mvalues)){
m = mvalues[idx]
rf_fit = randomForest(Salary ~ ., mtry = m, data = Hitters_train)
oob_errors[idx] = rf_fit$mse[ntree]
}
tibble(m = mvalues, oob_err = oob_errors) %>%
ggplot(aes(x = m, y = oob_err)) +
geom_line() + geom_point() +
scale_x_continuous(breaks = mvalues) +
theme_bw()
rf_fit = randomForest(Salary ~ ., data = Hitters_train)
rf_fit$importance
varImpPlot(rf_fit)
rf_fit = randomForest(Salary ~ ., importance = TRUE, data = Hitters_train)
rf_fit$importance
varImpPlot(rf_fit)
rf_predictions = predict(rf_fit, newdata = Hitters_test)
rf_predictions
mean((rf_predictions - Hitters_test$Salary)^2)
knitr::opts_chunk$set(echo = TRUE)
#load the rda file
load(file = "37323-0001-Data.rda")
#load the rda file
load(file = "37323-0001-Data.rda")
knitr::opts_chunk$set(echo = TRUE)
#load the rda file
load(file = "37323-0001-Data.rda")
#load the rda file
load(file = "37323-0001-Data.rda")
#load the rda file
load(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
View(da37323.0001)
lemas <- readRDS(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
#load the rda file
lemas=load(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
file.exists("/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
filename <- file.choose("37323-0001-Data")
lemas <- readRDS(filename)
filename <- file.choose("37323-0001-Data")
lemas <- readRDS(filename)
#load the rda file
load(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
#load the rda file
load(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/final-project/lemas2016/37323-0001-Data.rda")
View(da37323.0001)
url = "https://raw.githubusercontent.com/JieYingWu/COVID-19_US_County-level_Summaries/master/data/counties.csv"
jieyingwu = read_csv(url)
library(tidyverse)
library(tidyverse)
url = "https://raw.githubusercontent.com/JieYingWu/COVID-19_US_County-level_Summaries/master/data/counties.csv"
jieyingwu = read_csv(url)
jieyingwu
library(tidyverse)
url = "https://raw.githubusercontent.com/JieYingWu/COVID-19_US_County-level_Summaries/master/data/counties.csv"
jieyingwu = read_csv(url)
View(jieyingwu)
load(file ="/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/final-project/jailsurvey/37392-0001-Data.rda")
knitr::opts_chunk$set(echo = TRUE)
load(file ="/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/final-project/jailsurvey/37392-0001-Data.rda")
View(da37392.0001)
jieyingwu
view(jieyingwu)
=======
library(stringr)
library(gbm)
library(glmnetUtils) # boosting
library(randomForest)
# install.packages("scales")              # dependency of plot_glmnet
source("functions/plot_glmnet.R")
theft_train = read_csv("../data/clean/theft_train.csv")
theft_test = read_csv("../data/clean/theft_test.csv")
set.seed(471) # set seed for reproducibility
ridge_fit = cv.glmnet(theftrate ~ .-fips -state -county,  # formula notation, as usual
alpha = 0,                 # alpha = 0 for ridge
nfolds = 10,               # number of folds
data = theft_train)   # data to run ridge on
plot(ridge_fit)
plot_glmnet(ridge_fit, theft_train, features_to_plot = 8)
set.seed(471) # set seed before cross-validation for reproducibility
lasso_fit = cv.glmnet(theftrate ~. -state -county -fips , alpha = 1, nfolds = 10, data = theft_train)
plot(lasso_fit)
lambda_lasso = lasso_fit$lambda.1se
sprintf("The value of lambda based on the one-standard-error rule: %f",
lambda_lasso)
num_features = lasso_fit$nzero[lasso_fit$lambda == lasso_fit$lambda.1se]
sprintf("The number of features (excluding intercept) selected (1se): %i",
num_features)
extract_std_coefs(lasso_fit, theft_train) %>%
filter(coefficient != 0) %>% arrange(desc(coefficient))
plot_glmnet(lasso_fit, theft_train)
View(theft_train)
plot(gbm_3, i.var = "saversperhouses", n.trees =
optimal_num_trees)
>>>>>>> 6c497d1c2dba5cad3d9eddea537d6d18100ca131
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(readr)
library(readxl)
library(usdata)
library(tm)
library(stringr)
library(gbm)
library(glmnetUtils) # boosting
library(randomForest)
# install.packages("scales")              # dependency of plot_glmnet
source("functions/plot_glmnet.R")
theft_train = read_csv("../data/clean/theft_train.csv")
theft_test = read_csv("../data/clean/theft_test.csv")
set.seed(471) # set seed for reproducibility
ridge_fit = cv.glmnet(theftrate ~ .-fips -state -county,  # formula notation, as usual
alpha = 0,                 # alpha = 0 for ridge
nfolds = 10,               # number of folds
data = theft_train)   # data to run ridge on
plot(ridge_fit)
plot_glmnet(ridge_fit, theft_train, features_to_plot = 8)
set.seed(471) # set seed before cross-validation for reproducibility
lasso_fit = cv.glmnet(theftrate ~. -state -county -fips , alpha = 1, nfolds = 10, data = theft_train)
plot(lasso_fit)
lambda_lasso = lasso_fit$lambda.1se
sprintf("The value of lambda based on the one-standard-error rule: %f",
lambda_lasso)
num_features = lasso_fit$nzero[lasso_fit$lambda == lasso_fit$lambda.1se]
sprintf("The number of features (excluding intercept) selected (1se): %i",
num_features)
extract_std_coefs(lasso_fit, theft_train) %>%
filter(coefficient != 0) %>% arrange(desc(coefficient))
plot_glmnet(lasso_fit, theft_train)
elnet_fit = cva.glmnet(theftrate ~ . -fips -county -state, # formula notation, as usual
nfolds = 10,               # number of folds
data = theft_train)   # data to run on
elnet_fit$alpha
plot_cva_glmnet(elnet_fit)
elnet_fit_best = extract_best_elnet(elnet_fit)
elnet_fit_best$alpha
plot(elnet_fit_best)
plot_glmnet(elnet_fit_best, theft_train)
plot_glmnet(elnet_fit_best, theft_train, features_to_plot = 10)
extract_std_coefs(elnet_fit_best, theft_train) %>%
filter(coefficient != 0) %>% arrange(desc(abs(coefficient)))
set.seed(471) # set seed for reproducibility
final_elnet_fit = cv.glmnet(theftrate ~ .-fips -state -county,  # formula notation, as usual
alpha = elnet_fit_best$alpha,
nfolds = 10,               # number of folds
data = theft_train)   # data to run ridge on
set.seed(471) # set seed for reproducibility
mvalues = seq(4,16, by = 1)
oob_errors = numeric(length(mvalues))
ntree = 500
for(idx in 1:length(mvalues)){
m = mvalues[idx]
rf_fit = randomForest(theftrate ~. -fips -state - county, mtry = m, data = theft_train)
oob_errors[idx] = rf_fit$mse[ntree]
}
tibble(m = mvalues, oob_err = oob_errors) %>%
ggplot(aes(x = m, y = oob_err)) +
geom_line() + geom_point() +
scale_x_continuous(breaks = mvalues) +
theme_bw()
set.seed(471) # set seed for reproducibility
<<<<<<< HEAD
rf_6 = randomForest(theftrate ~ .-fips, mtry = 6, data = theft_train)
rf_6$importance
varImpPlot(rf_6,n.var = 10)
set.seed(471) # set seed for reproducibility
rf_6 = randomForest(theftrate ~ .-fips, mtry = 4, data = theft_train)
rf_6$importance
varImpPlot(rf_6,n.var = 10)
library(tidyverse)
library(glmnetUtils)
source("../code/functions/plot_glmnet.R")
theftdata=read_csv("../data/clean/dataclean.csv")
set.seed(471) # set seed for reproducibility
train_samples = sample(1:nrow(theftdata), round(0.8*nrow(theftdata)))
theft_train = theftdata %>% filter(row_number() %in% train_samples) %>% select(-county,-state)
theft_test = theftdata %>% filter(!(row_number() %in% train_samples))%>% select(-county,-state)
set.seed(471) # set seed for reproducibility
ridge_fit = cv.glmnet(theftrate ~ .-fips,  # formula notation, as usual
=======
rf_11 = randomForest(theftrate ~ .-fips -state -county, mtry = 11, data = theft_train)
rf_11$importance
varImpPlot(rf_11,n.var = 10)
set.seed(471) # for reproducibility (DO NOT CHANGE)
# TODO: Fit random forest with interaction depth 1
gbm_1 = gbm(theftrate ~ . -fips -state -county,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = 1,
shrinkage = 0.1,
cv.folds = 5,
data = theft_train)
set.seed(471) # for reproducibility (DO NOT CHANGE)
# TODO: Fit random forest with interaction depth 2
gbm_2 = gbm(theftrate ~ . -fips -state -county,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = 2,
shrinkage = 0.1,
cv.folds = 5,
data = theft_train)
set.seed(471) # for reproducibility (DO NOT CHANGE)
# TODO: Fit random forest with interaction depth 3
gbm_3 = gbm(theftrate ~ . -fips -state -county,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = 3,
shrinkage = 0.1,
cv.folds = 5,
data = theft_train)
ntrees = 1000
cv_errors = bind_rows(
tibble(ntree = 1:ntrees, cv_err = gbm_1$cv.error, Depth = 1),
tibble(ntree = 1:ntrees, cv_err = gbm_2$cv.error, Depth = 2),
tibble(ntree = 1:ntrees, cv_err = gbm_3$cv.error, Depth = 3)
) %>% mutate(Depth = factor(Depth))
# plot CV errors
mins = cv_errors %>% group_by(Depth) %>% summarise(min_err = min(cv_err))
gbm.perf(gbm_3, plot.it = FALSE)
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = Depth)) +
geom_line() + theme_bw() +
geom_hline(aes(yintercept = min_err, color = Depth),
data = mins, linetype = "dashed") +
labs(y = "CV Error", x = "Trees") + scale_y_log10()
gbm_fit_optimal = gbm_3
optimal_num_trees = gbm.perf(gbm_3, plot.it = FALSE)
summary(gbm_3, n.trees = optimal_num_trees, plotit = FALSE) %>% tibble() %>%
head(10)
plot(gbm_3, i.var = "housing_density",
n.trees = optimal_num_trees)
plot(gbm_3, i.var = "poor_fair_health",
n.trees = optimal_num_trees)
plot(gbm_3, i.var = "pertrump", n.trees =
optimal_num_trees)
plot(gbm_3, i.var = "PctEmpFIRE", n.trees =
optimal_num_trees)
plot(gbm_3, i.var = "saversperhouses", n.trees =
optimal_num_trees)
plot(gbm_3, i.var = "pop_density", n.trees =
optimal_num_trees)
plot(gbm_3, i.var = "unemp_bens_possible", n.trees =
optimal_num_trees)
# ridge prediction error
ridge_predictions = predict(ridge_fit,
newdata = theft_test,
s = "lambda.1se") %>% as.numeric()
ridge_RMSE = sqrt(mean((ridge_predictions-theft_test$theftrate)^2))
# lasso prediction error
lasso_predictions = predict(lasso_fit,
newdata = theft_test,
s = "lambda.1se") %>%
as.numeric()
lasso_RMSE = sqrt(mean((lasso_predictions-theft_test$theftrate)^2))
# elnet prediction error
elnet_predictions = predict(final_elnet_fit,
newdata = theft_test,
s = "lambda.1se") %>%
as.numeric()
elnet_RMSE = sqrt(mean((elnet_predictions-theft_test$theftrate)^2))
# intercept-only prediction error
training_mean_response = mean(theft_test$theftrate)
constant_RMSE = sqrt(mean((training_mean_response-theft_test$theftrate)^2))
#RF
rf_predictions = predict(rf_11, newdata = theft_test)
rf_RMSE = sqrt(mean((rf_predictions-theft_test$theftrate)^2))
#Boosting
gbm_predictions = predict(gbm_3, n.trees = optimal_num_trees,
newdata = theft_test)
gbm_RMSE = sqrt(mean((gbm_predictions-theft_test$theftrate)^2))
# print nice table
tibble(Ridge = ridge_RMSE, Lasso = lasso_RMSE, `Intercept-only` = constant_RMSE,
Elastic_Net = elnet_RMSE, Random_Forest = rf_RMSE, Boosting = gbm_RMSE) %>% pivot_longer(everything(), names_to = "Model", values_to = "Error")
tibble(Ridge = ridge_RMSE, Lasso = lasso_RMSE, `Intercept-only` = constant_RMSE,
"Elastic Net" = elnet_RMSE, "Random Forest" = rf_RMSE, Boosting = gbm_RMSE, Response_Mean) %>% pivot_longer(everything(), names_to = "Model", values_to = "Test Error")
tibble(Ridge = ridge_RMSE, Lasso = lasso_RMSE, `Intercept-only` = constant_RMSE,
Elastic_Net = elnet_RMSE, Random_Forest = rf_RMSE, Boosting = gbm_RMSE, Response_Mean) %>% pivot_longer(everything(), names_to = "Model", values_to = "Test Error")
tibble(Ridge = ridge_RMSE, Lasso = lasso_RMSE, `Intercept-only` = constant_RMSE,
Elastic_Net = elnet_RMSE, Random_Forest = rf_RMSE, Boosting = gbm_RMSE, Response_Mean = mean(theft_test$theftrate)) %>% pivot_longer(everything(), names_to = "Model", values_to = "Test Error")
tibble(Ridge = ridge_RMSE, Lasso = lasso_RMSE, `Intercept-only` = constant_RMSE,
Elastic_Net = elnet_RMSE, Random_Forest = rf_RMSE, Boosting = gbm_RMSE) %>% pivot_longer(everything(), names_to = "Model", values_to = "Test Error")
Response_Mean = mean(theft_test$theftrate)
Response_Mean = mean(theft_test$theftrate)
Response_Mean
set.seed(471) # set seed for reproducibility
rf_13 = randomForest(theftrate ~ .-fips -state -county, mtry = 13, data = theft_train)
rf_13$importance
varImpPlot(rf_11,n.var = 10)
varImpPlot(rf_13,n.var = 10)
# ridge prediction error
ridge_predictions = predict(ridge_fit,
newdata = theft_test,
s = "lambda.1se") %>% as.numeric()
ridge_RMSE = sqrt(mean((ridge_predictions-theft_test$theftrate)^2))
# lasso prediction error
lasso_predictions = predict(lasso_fit,
newdata = theft_test,
s = "lambda.1se") %>%
as.numeric()
lasso_RMSE = sqrt(mean((lasso_predictions-theft_test$theftrate)^2))
# elnet prediction error
elnet_predictions = predict(final_elnet_fit,
newdata = theft_test,
s = "lambda.1se") %>%
as.numeric()
elnet_RMSE = sqrt(mean((elnet_predictions-theft_test$theftrate)^2))
# intercept-only prediction error
training_mean_response = mean(theft_test$theftrate)
constant_RMSE = sqrt(mean((training_mean_response-theft_test$theftrate)^2))
#RF
rf_predictions = predict(rf_13, newdata = theft_test)
rf_RMSE = sqrt(mean((rf_predictions-theft_test$theftrate)^2))
#Boosting
gbm_predictions = predict(gbm_3, n.trees = optimal_num_trees,
newdata = theft_test)
gbm_RMSE = sqrt(mean((gbm_predictions-theft_test$theftrate)^2))
# print nice table
tibble(Ridge = ridge_RMSE, Lasso = lasso_RMSE, `Intercept-only` = constant_RMSE,
Elastic_Net = elnet_RMSE, Random_Forest = rf_RMSE, Boosting = gbm_RMSE) %>% pivot_longer(everything(), names_to = "Model", values_to = "Test RMSE")
plot_glmnet(elnet_fit_best, theft_train, features_to_plot = 10)
elnet_fit = cva.glmnet(theftrate ~ . -fips -county -state, # formula notation, as usual
nfolds = 10,               # number of folds
data = theft_train)   # data to run on
elnet_fit$alpha
plot_cva_glmnet(elnet_fit)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(readr)
library(readxl)
library(usdata)
library(tm)
library(stringr)
library(gbm)
library(glmnetUtils) # boosting
library(randomForest)
# install.packages("scales")              # dependency of plot_glmnet
source("functions/plot_glmnet.R")
theft_train = read_csv("../data/clean/theft_train.csv")
theft_test = read_csv("../data/clean/theft_test.csv")
set.seed(471) # set seed for reproducibility
ridge_fit = cv.glmnet(theftrate ~ .-fips -state -county,  # formula notation, as usual
>>>>>>> 6c497d1c2dba5cad3d9eddea537d6d18100ca131
alpha = 0,                 # alpha = 0 for ridge
nfolds = 10,               # number of folds
data = theft_train)   # data to run ridge on
plot(ridge_fit)
<<<<<<< HEAD
plot_glmnet(ridge_fit, crime_data_train, features_to_plot = 10)
plot_glmnet(ridge_fit, theft_train, features_to_plot = 10)
plot_glmnet(ridge_fit, theft_train, features_to_plot = 7)
plot_glmnet(ridge_fit, theft_train, features_to_plot = 8)
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
setwd("/Users/diyangchu/Documents/2-grad@Penn/STAT471/theft-crime-2020/code")
library(tidyverse)
library(glmnetUtils)
source("/Users/diyangchu/Documents/2-grad@Penn/STAT471/theft-crime-2020/code/functions/plot_glmnet.R")
theft_train = read_csv("../data/clean/theft_train.csv")
theft_test = read_csv("../data/clean/theft_test.csv")
theft_train_model = theft_train %>% select(-fips, -state, -county, -households)
theft_test_pred = theft_test%>%select(-fips, -state, -county, -households)
set.seed(471) # set seed for reproducibility
ridge_fit = cv.glmnet(theftrate ~ .,  # formula notation, as usual
alpha = 0,                 # alpha = 0 for ridge
nfolds = 10,               # number of folds
data = theft_train_model)   # data to run ridge on
plot(ridge_fit)
plot_glmnet(ridge_fit, theft_train_model, features_to_plot = 8)
ridge_predictions = predict(ridge_fit,
newdata = theft_test_pred,
s = "lambda.1se") %>% as.numeric()
RMSE_ridge = sqrt(mean((ridge_predictions - theft_test_pred$theftrate)^2))
=======
plot_glmnet(ridge_fit, theft_train, features_to_plot = 8)
set.seed(471) # set seed before cross-validation for reproducibility
lasso_fit = cv.glmnet(theftrate ~. -state -county -fips , alpha = 1, nfolds = 10, data = theft_train)
plot(lasso_fit)
lambda_lasso = lasso_fit$lambda.1se
sprintf("The value of lambda based on the one-standard-error rule: %f",
lambda_lasso)
num_features = lasso_fit$nzero[lasso_fit$lambda == lasso_fit$lambda.1se]
sprintf("The number of features (excluding intercept) selected (1se): %i",
num_features)
extract_std_coefs(lasso_fit, theft_train) %>%
filter(coefficient != 0) %>% arrange(desc(coefficient))
plot_glmnet(lasso_fit, theft_train)
elnet_fit = cva.glmnet(theftrate ~ . -fips -county -state, # formula notation, as usual
nfolds = 10,               # number of folds
data = theft_train)   # data to run on
elnet_fit$alpha
plot_cva_glmnet(elnet_fit)
elnet_fit = cva.glmnet(theftrate ~ .-fips -county -state, # formula notation, as usual
nfolds = 10,               # number of folds
data = theft_train)   # data to run on
elnet_fit$alpha
plot_cva_glmnet(elnet_fit)
elnet_fit_best = extract_best_elnet(elnet_fit)
elnet_fit_best$alpha
plot(elnet_fit_best)
plot_glmnet(elnet_fit_best, theft_train)
plot_glmnet(elnet_fit_best, theft_train, features_to_plot = 10)
extract_std_coefs(elnet_fit_best, theft_train) %>%
filter(coefficient != 0) %>% arrange(desc(abs(coefficient)))
set.seed(471) # set seed for reproducibility
final_elnet_fit = cv.glmnet(theftrate ~ .-fips -state -county,  # formula notation, as usual
alpha = elnet_fit_best$alpha,
nfolds = 10,               # number of folds
data = theft_train)   # data to run ridge on
# ridge prediction error
ridge_predictions = predict(ridge_fit,
newdata = theft_test,
s = "lambda.1se") %>% as.numeric()
ridge_RMSE = sqrt(mean((ridge_predictions-theft_test$theftrate)^2))
# lasso prediction error
lasso_predictions = predict(lasso_fit,
newdata = theft_test,
s = "lambda.1se") %>%
as.numeric()
lasso_RMSE = sqrt(mean((lasso_predictions-theft_test$theftrate)^2))
# elnet prediction error
elnet_predictions = predict(final_elnet_fit,
newdata = theft_test,
s = "lambda.1se") %>%
as.numeric()
elnet_RMSE = sqrt(mean((elnet_predictions-theft_test$theftrate)^2))
# intercept-only prediction error
training_mean_response = mean(theft_test$theftrate)
constant_RMSE = sqrt(mean((training_mean_response-theft_test$theftrate)^2))
#RF
rf_predictions = predict(rf_13, newdata = theft_test)
rf_RMSE = sqrt(mean((rf_predictions-theft_test$theftrate)^2))
#Boosting
gbm_predictions = predict(gbm_3, n.trees = optimal_num_trees,
newdata = theft_test)
gbm_RMSE = sqrt(mean((gbm_predictions-theft_test$theftrate)^2))
# print nice table
tibble(Ridge = ridge_RMSE, Lasso = lasso_RMSE, `Intercept-only` = constant_RMSE,
Elastic_Net = elnet_RMSE, Random_Forest = rf_RMSE, Boosting = gbm_RMSE) %>% pivot_longer(everything(), names_to = "Model", values_to = "Test RMSE")
plot(ridge_fit)
plot_glmnet(ridge_fit, theft_train, features_to_plot = 8)
set.seed(471) # set seed before cross-validation for reproducibility
lasso_fit = cv.glmnet(theftrate ~. -state -county -fips , alpha = 1, nfolds = 10, data = theft_train)
plot(lasso_fit)
lambda_lasso = lasso_fit$lambda.1se
sprintf("The value of lambda based on the one-standard-error rule: %f",
lambda_lasso)
lambda_lasso = lasso_fit$lambda.1se
sprintf("The value of lambda based on the one-standard-error rule: %f",
lambda_lasso)
num_features = lasso_fit$nzero[lasso_fit$lambda == lasso_fit$lambda.1se]
sprintf("The number of features (excluding intercept) selected (1se): %i",
num_features)
extract_std_coefs(lasso_fit, theft_train) %>%
filter(coefficient != 0) %>% arrange(desc(coefficient))
plot_glmnet(lasso_fit, theft_train)
elnet_fit = cva.glmnet(theftrate ~ .-fips -county -state, # formula notation, as usual
nfolds = 10,               # number of folds
data = theft_train)   # data to run on
elnet_fit$alpha
elnet_fit$alpha
elnet_fit$alpha
elnet_fit$alpha
plot_cva_glmnet(elnet_fit)
elnet_fit_best = extract_best_elnet(elnet_fit)
elnet_fit_best$alpha
plot(elnet_fit_best)
plot_glmnet(elnet_fit_best, theft_train)
plot_glmnet(elnet_fit_best, theft_train, features_to_plot = 10)
extract_std_coefs(elnet_fit_best, theft_train) %>%
filter(coefficient != 0) %>% arrange(desc(abs(coefficient)))
set.seed(471) # set seed for reproducibility
final_elnet_fit = cv.glmnet(theftrate ~ .-fips -state -county,  # formula notation, as usual
alpha = elnet_fit_best$alpha,
nfolds = 10,               # number of folds
data = theft_train)   # data to run ridge on
set.seed(471) # set seed for reproducibility
final_elnet_fit = cv.glmnet(theftrate ~ .-fips -state -county,  # formula notation, as usual
alpha = elnet_fit_best$alpha,
nfolds = 10,               # number of folds
data = theft_train)   # data to run ridge on
set.seed(471) # set seed for reproducibility
mvalues = seq(4,16, by = 1)
oob_errors = numeric(length(mvalues))
ntree = 500
for(idx in 1:length(mvalues)){
m = mvalues[idx]
rf_fit = randomForest(theftrate ~. -fips -state - county, mtry = m, data = theft_train)
oob_errors[idx] = rf_fit$mse[ntree]
}
OLS_Predictions = predict((lm(theftrate ~ . -state, county, fips, data = theft_train), newdata = theft_test))
OLS_Predictions = predict((lm(theftrate ~ . -state, -county -fips, data = theft_train), newdata = theft_test))
OLS_Predictions = predict((lm(theftrate ~ . -state -county -fips, data = theft_train), newdata = theft_test))
OLS_Predictions = predict((lm(theftrate ~ . -state -county -fips, data = theft_train)), newdata = theft_test)
theft_train = read_csv("../data/clean/theft_train.csv") %>% select(-fips, -county, -state)
theft_test = read_csv("../data/clean/theft_test.csv") %>% select(-fips, -county, -state)
# OLS
OLS_Predictions = predict((lm(theftrate ~ .-state -county -fips, data = theft_train)), newdata = theft_test)
lm(theftrate ~ .-state -county -fips, data = theft_train)
# OLS
OLS_Predictions = predict((lm(theftrate ~ ., data = theft_train)), newdata = theft_test)
# ridge prediction error
ridge_predictions = predict(ridge_fit,
newdata = theft_test,
s = "lambda.1se") %>% as.numeric()
ridge_RMSE = sqrt(mean((ridge_predictions-theft_test$theftrate)^2))
# lasso prediction error
lasso_predictions = predict(lasso_fit,
newdata = theft_test,
s = "lambda.1se") %>%
as.numeric()
lasso_RMSE = sqrt(mean((lasso_predictions-theft_test$theftrate)^2))
# elnet prediction error
elnet_predictions = predict(final_elnet_fit,
newdata = theft_test,
s = "lambda.1se") %>%
as.numeric()
elnet_RMSE = sqrt(mean((elnet_predictions-theft_test$theftrate)^2))
# intercept-only prediction error
training_mean_response = mean(theft_test$theftrate)
constant_RMSE = sqrt(mean((training_mean_response-theft_test$theftrate)^2))
#RF
rf_predictions = predict(rf_13, newdata = theft_test)
OLS_Predictions = predict((lm(theftrate ~ ., data = theft_train)), newdata = theft_test)
OLS_RMSE = sqrt(mean((OLS_predictions-theft_test$theftrate)^2))
# OLS
testforOLS = theft_test %>% select(-fips, -county, -state)
theft_train = read_csv("../data/clean/theft_train.csv")
theft_test = read_csv("../data/clean/theft_test.csv")
# OLS
testforOLS = theft_test %>% select(-fips, -county, -state)
OLS_Predictions = predict((lm(theftrate ~ ., data = theft_train)), newdata = testforOLS)
# OLS
testforOLS = theft_test %>% select(-fips, -county, -state)
OLS_Predictions = predict((lm(theftrate ~ . -county -state -fips, data = theft_train)), newdata = testforOLS)
# OLS
testforOLS = theft_test %>% select(-fips, -county, -state)
OLS_Predictions = predict((lm(theftrate ~ . -county -state -fips, data = theft_train)), newdata = theft_test)
# OLS
testforOLS = theft_test %>% select(-fips)
OLS_Predictions = predict((lm(theftrate ~ . -county -state -fips, data = theft_train)), newdata = testforOLS)
# OLS
testforOLS = theft_test %>% select(-fips)
lm_fit = lm(theftrate ~ . -county -state -fips, data = theft_train)
OLS_Predictions = predict(lm_fit, newdata = testforOLS)
# OLS
testforOLS = theft_test %>% select(-fips)
lm_fit = lm(theftrate ~ . -county -state -fips, data = theft_train)
OLS_Predictions = predict(lm_fit, newdata = theft_test)
# OLS
testforOLS = theft_test %>% select(-fips,  -county, -state)
lm_fit = lm(theftrate ~ . -county -state -fips, data = theft_train)
OLS_Predictions = predict(lm_fit, newdata = theft_test)
# OLS
testforOLS = theft_test %>% select(-fips,  -county, -state)
lm_fit = lm(theftrate ~ . -county -state -fips, data = testforOLS)
# OLS
testforOLS = theft_test %>% select(-county, -state)
trainforOLS = theft_train %>% select(-county, -state)
lm_fit = lm(theftrate ~ ., data = testforOLS)
OLS_Predictions = predict(lm_fit, newdata = theft_test)
OLS_RMSE = sqrt(mean((OLS_predictions-testforOLS$theftrate)^2))
# OLS
testforOLS = theft_test %>% select(-fips)
trainforOLS = theft_train %>% select(-fips)
lm_fit = lm(theftrate ~ .-county -state, data = testforOLS)
OLS_Predictions = predict(lm_fit, newdata = theft_test)
OLS_RMSE = sqrt(mean((OLS_predictions-testforOLS$theftrate)^2))
# OLS
testforOLS = theft_test %>% select(-county, -state)
trainforOLS = theft_train %>% select(-county, -state) %>% mutate(fips = as.factor(fips))
lm_fit = lm(theftrate ~ ., data = testforOLS)
OLS_Predictions = predict(lm_fit, newdata = theft_test)
OLS_RMSE = sqrt(mean((OLS_predictions-testforOLS$theftrate)^2))
# OLS
testforOLS = theft_test %>% select(-county, -state)%>% mutate(fips = as.factor(fips))
trainforOLS = theft_train %>% select(-county, -state) %>% mutate(fips = as.factor(fips))
lm_fit = lm(theftrate ~ .-fips, data = testforOLS)
OLS_Predictions = predict(lm_fit, newdata = theft_test)
# OLS
testforOLS = theft_test %>% select(-county, -state)%>% mutate(fips = as.character(fips))
trainforOLS = theft_train %>% select(-county, -state) %>% mutate(fips = as.character(fips))
lm_fit = lm(theftrate ~ .-fips, data = testforOLS)
OLS_Predictions = predict(lm_fit, newdata = theft_test)
# OLS
testforOLS = theft_test %>% select(-county, -state)
trainforOLS = theft_train %>% select(-county, -state)
lm_fit = lm(theftrate ~ .-fips, data = testforOLS)
OLS_Predictions = predict(lm_fit, newdata = theft_test)
OLS_RMSE = sqrt(mean((OLS_predictions-testforOLS$theftrate)^2))
# ridge prediction error
ridge_predictions = predict(ridge_fit,
newdata = theft_test,
s = "lambda.1se") %>% as.numeric()
ridge_RMSE = sqrt(mean((ridge_predictions-theft_test$theftrate)^2))
# lasso prediction error
lasso_predictions = predict(lasso_fit,
newdata = theft_test,
s = "lambda.1se") %>%
as.numeric()
lasso_RMSE = sqrt(mean((lasso_predictions-theft_test$theftrate)^2))
# elnet prediction error
elnet_predictions = predict(final_elnet_fit,
newdata = theft_test,
s = "lambda.1se") %>%
as.numeric()
elnet_RMSE = sqrt(mean((elnet_predictions-theft_test$theftrate)^2))
# intercept-only prediction error
training_mean_response = mean(theft_test$theftrate)
constant_RMSE = sqrt(mean((training_mean_response-theft_test$theftrate)^2))
#RF
rf_predictions = predict(rf_13, newdata = theft_test)
rf_RMSE = sqrt(mean((rf_predictions-theft_test$theftrate)^2))
#Boosting
gbm_predictions = predict(gbm_3, n.trees = optimal_num_trees,
newdata = theft_test)
gbm_RMSE = sqrt(mean((gbm_predictions-theft_test$theftrate)^2))
# print nice table
tibble(Ridge = ridge_RMSE, Lasso = lasso_RMSE, `Intercept-only` = constant_RMSE,
Elastic_Net = elnet_RMSE, Random_Forest = rf_RMSE, Boosting = gbm_RMSE) %>% pivot_longer(everything(), names_to = "Model", values_to = "Test RMSE")
>>>>>>> 6c497d1c2dba5cad3d9eddea537d6d18100ca131
