colour = "red", linetype ="longdash", size = .8)+
labs(x = "Acceptance Rate", y = "Count") +
theme_bw()
median(college_train$Accept)
college_train %>% arrange(Accept) %>%           # arrange in increasing order
head(1) %>% select(Name,Accept)
college_train%>%ggplot(aes(x=Grad.Rate, y=Accept)) +
geom_point()+
labs(y = "Acceptance Rate", x = "Graduation Rate") +
theme_bw()
college_train%>%ggplot(aes(x=Top10perc, y=Accept)) +
geom_point()+
labs(y = "Acceptance Rate", x = "Percentage of Top 10% Students") +
theme_bw()
college_train%>%ggplot(aes(x=Room.Board, y=Accept)) +
geom_point()+
labs(y = "Acceptance Rate", x = "Room & Board Costs") +
theme_bw()
college_train %>%
filter(Name=="Harvard University")%>%
select(Top10perc)
college_train %>%
arrange(desc(Top10perc))%>%
head(1)%>%
select(Name, Accept, Top10perc)
college_train = college_train %>% select(-Name)
college_test = college_test %>% select(-Name)
lm_fit=lm(Accept~., data = college_train)
summary(lm_fit)
set.seed(3) # set seed before cross-validation for reproducibility
ridge_fit = cv.glmnet(Accept ~ .,  # formula notation, as usual
alpha = 0,                 # alpha = 0 for ridge
nfolds = 10,               # number of folds
data = college_train)   # data to run ridge on
#get CV plot
plot(ridge_fit)
# lambda based on one-standard-error rule
ridge_fit$lambda.1se
set.seed(3) # set seed before cross-validation for reproducibility
ridge_fit = cv.glmnet(Accept ~ .,  # formula notation, as usual
alpha = 0,                 # alpha = 0 for ridge
nfolds = 10,               # number of folds
data = college_train)   # data to run ridge on
#get CV plot
plot(ridge_fit)
# lambda based on one-standard-error rule
ridge_fit$lambda.1se
length(ridge_fit$lambda)
plot_glmnet(ridge_fit, college_train, features_to_plot = 6)
coeffs = tibble(lm_coef = coef(lm_fit)[-1],
ridge_coef = coef(ridge_fit, s = "lambda.1se")[-1,1],
features = names(coef(lm_fit)[-1]))
coeffs
coeffs %>%
filter((lm_coef>0 & ridge_coef<0)|(lm_coef<0 & ridge_coef>0))%>%
summarise(num_diff_sign=n())
coeffs %>%
filter(abs(lm_coef)<abs(ridge_coef))%>%
summarise(num_smaller=n())
set.seed(5) # set seed before cross-validation for reproducibility
lasso_fit = cv.glmnet(Accept ~ .,
alpha = 1,                 # alpha = 1 for lasso
nfolds = 10,               # number of folds
data = college_train)   # data to run lasso on
#get the CV plot
plot(lasso_fit)
plot_glmnet(lasso_fit, college_train)
install.packages(c("rmarkdown", "tidyr", "tinytex"))
install.packages(c("stringi", "systemfonts", "tibble", "vroom", "xfun"))
install.packages(c("stringi", "systemfonts", "tibble", "vroom", "xfun"))
install.packages(c("stringi", "systemfonts", "tibble", "vroom", "xfun"))
install.packages(c("cpp11", "data.table", "digest", "hms", "ISLR2", "knitr"))
install.packages(c("openssl", "pillar", "readr", "rlang", "rvest"))
install.packages(c("lattice", "lifecycle", "lubridate", "mgcv", "mime", "nlme"))
options(scipen = 0, digits = 3)  # controls number of significant digits printed
exp(0.0166)
set.seed(1)
sample(1:10,1)
RNGkind()
RNGkind(sample.kind = "Rejection")
RNGkind()
set.seed(1)
sample(1:10,1)
options(scipen = 0, digits = 3)  # controls number of significant digits printed
library(kableExtra) # for printing tables
library(cowplot)    # for side by side plots
library(glmnet)     # to run ridge and lasso
library(ISLR2)      # necessary for College data
library(pROC)       # for ROC curves
library(tidyverse)
library(glmnetUtils)
# install.packages("scales")              # dependency of plot_glmnet
source("../../functions/plot_glmnet.R")
hd_data = read_csv("../../data/Framingham.dat",col_names = TRUE,col_types = "iifiiiii")
hd_data=as_tibble(hd_data)
hd_data = hd_data %>% rename(c('HD'='Heart Disease?')) %>% na.omit()
hd_data %>%
summarise(mean(HD))
set.seed(5) # seed set for reproducibility (DO NOT CHANGE)
n = nrow(hd_data)
train_samples = sample(1:n, round(0.8*n))
hd_train = hd_data %>% filter(row_number() %in% train_samples)
hd_test = hd_data %>% filter(!(row_number() %in% train_samples))
ggplot(hd_train, aes(x = AGE)) +
geom_histogram(binwidth =1, colour = "blue", fill = "white")+
geom_vline(aes(xintercept = median(AGE)),
colour = "red", linetype ="longdash", size = .8)+
labs(y = "Count", x = "Age") +
theme_bw()
ggplot(hd_train) +
geom_boxplot(aes(x=as.factor(HD), y=SBP, fill=as.factor(HD)))+
labs(y = "SBP (systolic blood pressure)", x = "Heart Disease Condition") +
theme_bw()
hd_train_subset = hd_train %>%
filter(CIG ==40,CHOL >=260) %>% select(HD,SBP)
hd_train_subset
glm_sub_fit = glm(HD ~ SBP,
family = "binomial",
data = hd_train_subset)
coef(glm_sub_fit)
b0=-10.1427
b1=0.0737
exp(b0+b1*150)/(1+exp(b0+b1*150))
exp(b0+b1*130)/(1+exp(b0+b1*130))
exp(b0+b1*190)/(1+exp(b0+b1*190))
exp(b0+b1*142)/(1+exp(b0+b1*142))
likelihood = function(beta_1)((exp((-10.1427)+beta_1*150)/(1+exp((-10.1427)+beta_1*150)))*
(exp((-10.1427)+beta_1*130)/(1+exp((-10.1427)+beta_1*130)))*
(exp((-10.1427)+beta_1*190)/(1+exp((-10.1427)+beta_1*190)))*
(1/(1+exp((-10.1427)+beta_1*142)))*(1/(1+exp((-10.1427)+beta_1*130))))
ggplot(data.frame(x = c(0, 0.1)),aes(x = x)) +
stat_function(fun = likelihood)+
geom_vline(aes(xintercept = 0.0737),
colour = "red", linetype ="longdash", size = .8)+
labs(x = "beta_1", y = "Likelihood") +
theme_bw()
glm_fit = glm(HD ~ SBP,
family = "binomial",
data = hd_train)
coef(glm_fit)
exp(0.0166)
hd_train %>%
ggplot(aes(x = SBP, y = HD))+
geom_jitter(height = .05) +
geom_smooth(method = "glm",
formula = "y~x",
method.args = list(family = "binomial"),
se = FALSE) +
ylab("Prob(HD=1)") +
theme_bw()
glm_multi_fit = glm(HD~.,
family = "binomial",
data = hd_train)
coef(glm_multi_fit)
exp(0.06151*10)
logodds_mary=coef(glm_multi_fit)[1]+coef(glm_multi_fit)[2]*50+
coef(glm_multi_fit)[3]*1+coef(glm_multi_fit)[4]*110+
coef(glm_multi_fit)[5]*80+coef(glm_multi_fit)[6]*180+
coef(glm_multi_fit)[7]*105+coef(glm_multi_fit)[8]*0
prob_mary = exp(logodds_mary)/(1+exp(logodds_mary))
prob_mary
coef(glm_multi_fit) %>% pull()
coef(glm_multi_fit) %>% as.numeric()
logodds_mary=coef(glm_multi_fit)[1]+coef(glm_multi_fit)[2]*50+
coef(glm_multi_fit)[3]*1+coef(glm_multi_fit)[4]*110+
coef(glm_multi_fit)[5]*80+coef(glm_multi_fit)[6]*180+
coef(glm_multi_fit)[7]*105+coef(glm_multi_fit)[8]*0
prob_mary = exp(logodds_mary)/(1+exp(logodds_mary))
prob_mary
coef_vec= coef(glm_multi_fit) %>% as.numeric()
coef_vec[1]
logodds_mary=coef_vec[1]+coef_vec[2]*50+
coef_vec[3]*1+coef_vec[4]*110+
coef_vec[5]*80+coef_vec[6]*180+
coef_vec[7]*105+coef_vec[8]*0
prob_mary = exp(logodds_mary)/(1+exp(logodds_mary))
prob_mary
library(rpart)             # install.packages("rpart")
library(rpart.plot)        # install.packages("rpart.plot")
library(tidyverse)
Hitters = ISLR2::Hitters %>%
as_tibble() %>%
filter(!is.na(Salary)) %>%   # remove NA values (in general not necessary)
mutate(Salary = log(Salary)) # log-transform the salary
Hitters
RNGkind
RNGkind
RNGkind(sample.kind = "Rejection")
set.seed(1) # set seed for reproducibility
train_samples = sample(1:nrow(Hitters), round(0.8*nrow(Hitters)))
Hitters_train = Hitters %>% filter(row_number() %in% train_samples)
Hitters_test = Hitters %>% filter(!(row_number() %in% train_samples))
Hitters_train %>% ggplot(aes(x = CAtBat, y = Hits, colour = Salary)) +
geom_point() + theme_bw()
tree_fit = rpart(Salary ~ ., data = Hitters_train)
rpart.plot(tree_fit)
tree_fit
tree_fit$variable.importance
# this code is not meant to be run
control = rpart.control(minsplit = 20, minbucket = round(minsplit/3))
tree_fit$variable.importance
tree_fit = rpart(Salary ~ ., data = Hitters_train)
rpart.plot(tree_fit)
tree_fit
tree_fit$variable.importance
# this code is not meant to be run
control = rpart.control(minsplit = 20, minbucket = round(minsplit/3))
library(rpart)             # install.packages("rpart")
library(rpart.plot)        # install.packages("rpart.plot")
library(tidyverse)
Hitters = ISLR2::Hitters %>%
as_tibble() %>%
filter(!is.na(Salary)) %>%   # remove NA values (in general not necessary)
mutate(Salary = log(Salary)) # log-transform the salary
Hitters
set.seed(1) # set seed for reproducibility
train_samples = sample(1:nrow(Hitters), round(0.8*nrow(Hitters)))
Hitters_train = Hitters %>% filter(row_number() %in% train_samples)
Hitters_test = Hitters %>% filter(!(row_number() %in% train_samples))
Hitters_train %>% ggplot(aes(x = CAtBat, y = Hits, colour = Salary)) +
geom_point() + theme_bw()
tree_fit = rpart(Salary ~ ., data = Hitters_train)
rpart.plot(tree_fit)
tree_fit
tree_fit$variable.importance
# this code is not meant to be run
control = rpart.control(minsplit = 20, minbucket = round(minsplit/3))
tree_fit_2 = rpart(Salary ~ .,
control = rpart.control(minsplit = 80),
data = Hitters_train)
rpart.plot(tree_fit_2)
url = "https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/master/Notebooks/Data/Heart.csv"
Heart = read_csv(url) %>% select(-...1)
Heart
set.seed(1) # set seed for reproducibility
train_samples = sample(1:nrow(Heart), round(0.8*nrow(Heart)))
Heart_train = Heart %>% filter(row_number() %in% train_samples)
Heart_test = Heart %>% filter(!(row_number() %in% train_samples))
tree_fit = rpart(AHD ~ .,
method = "class",              # classification
parms = list(split = "gini"),  # Gini index for splitting
data = Heart_train)
rpart.plot(tree_fit)
pred = predict(tree_fit, newdata = Heart_test)
pred %>% head()
pred = predict(tree_fit, newdata = Heart_test, type = "class")
pred
Hitters = ISLR2::Hitters %>%
as_tibble() %>%
filter(!is.na(Salary)) %>%
mutate(Salary = log(Salary)) # log-transform the salary
library(randomForest)       # install.packages("randomForest")
library(tidyverse)
RNGkind(sample.kind = "Rejection")
Hitters = ISLR2::Hitters %>%
as_tibble() %>%
filter(!is.na(Salary)) %>%
mutate(Salary = log(Salary)) # log-transform the salary
Hitters
set.seed(1) # set seed for reproducibility
train_samples = sample(1:nrow(Hitters), round(0.8*nrow(Hitters)))
Hitters_train = Hitters %>% filter(row_number() %in% train_samples)
Hitters_test = Hitters %>% filter(!(row_number() %in% train_samples))
rf_fit = randomForest(Salary ~ ., data = Hitters_train)
?randomForest
plot(rf_fit)
rf_fit = randomForest(Salary ~ ., mtry = 19, data = Hitters_train)
plot(rf_fit)
rf_3 = randomForest(Salary ~ ., mtry = 3, data = Hitters_train)
rf_6 = randomForest(Salary ~ ., mtry = 6, data = Hitters_train)
rf_19 = randomForest(Salary ~ ., mtry = 19, data = Hitters_train)
#MSE = OOB error
oob_errors = bind_rows(
tibble(ntree = 1:500, oob_err = rf_3$mse, m = 3),
tibble(ntree = 1:500, oob_err = rf_6$mse, m = 6),
tibble(ntree = 1:500, oob_err = rf_19$mse, m = 19)
)
oob_errors
oob_errors %>%
ggplot(aes(x = ntree, y = oob_err, colour = factor(m))) +
geom_line() + theme_bw()
# might want to cache this chunk!
mvalues = seq(1,19, by = 2)
oob_errors = numeric(length(mvalues))
ntree = 500
for(idx in 1:length(mvalues)){
m = mvalues[idx]
rf_fit = randomForest(Salary ~ ., mtry = m, data = Hitters_train)
oob_errors[idx] = rf_fit$mse[ntree]
}
tibble(m = mvalues, oob_err = oob_errors) %>%
ggplot(aes(x = m, y = oob_err)) +
geom_line() + geom_point() +
scale_x_continuous(breaks = mvalues) +
theme_bw()
rf_fit = randomForest(Salary ~ ., data = Hitters_train)
rf_fit$importance
varImpPlot(rf_fit)
rf_fit = randomForest(Salary ~ ., importance = TRUE, data = Hitters_train)
rf_fit$importance
varImpPlot(rf_fit)
rf_predictions = predict(rf_fit, newdata = Hitters_test)
rf_predictions
mean((rf_predictions - Hitters_test$Salary)^2)
knitr::opts_chunk$set(echo = TRUE)
#load the rda file
load(file = "37323-0001-Data.rda")
#load the rda file
load(file = "37323-0001-Data.rda")
knitr::opts_chunk$set(echo = TRUE)
#load the rda file
load(file = "37323-0001-Data.rda")
#load the rda file
load(file = "37323-0001-Data.rda")
#load the rda file
load(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
View(da37323.0001)
lemas <- readRDS(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
#load the rda file
lemas=load(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
file.exists("/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
filename <- file.choose("37323-0001-Data")
lemas <- readRDS(filename)
filename <- file.choose("37323-0001-Data")
lemas <- readRDS(filename)
#load the rda file
load(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/37323-0001-Data.rda")
#load the rda file
load(file = "/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/final-project/lemas2016/37323-0001-Data.rda")
View(da37323.0001)
url = "https://raw.githubusercontent.com/JieYingWu/COVID-19_US_County-level_Summaries/master/data/counties.csv"
jieyingwu = read_csv(url)
library(tidyverse)
library(tidyverse)
url = "https://raw.githubusercontent.com/JieYingWu/COVID-19_US_County-level_Summaries/master/data/counties.csv"
jieyingwu = read_csv(url)
jieyingwu
library(tidyverse)
url = "https://raw.githubusercontent.com/JieYingWu/COVID-19_US_County-level_Summaries/master/data/counties.csv"
jieyingwu = read_csv(url)
View(jieyingwu)
load(file ="/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/final-project/jailsurvey/37392-0001-Data.rda")
knitr::opts_chunk$set(echo = TRUE)
load(file ="/Users/diyangchu/Documents/2-grad@Penn/STAT471/stat-471-fall-2021/final-project/jailsurvey/37392-0001-Data.rda")
View(da37392.0001)
jieyingwu
view(jieyingwu)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(readr)
library(tidycensus)
library(rpart)
library(rpart.plot)
library(ipred)
library(readxl)
library(usdata)
library(tm)
library(stringr)
var <- load_variables(2019, "acs1", cache = TRUE)
census_api_key("75692c32a90eb5b381e167037e4341b90af4c5d3", overwrite = TRUE, install = TRUE)
census_data <- get_acs(geography = "county", variables = c(med_age = "B01002_001",
permale = "B01001_002",
bachplus = "B16010_041",
totalpop = "B01003_001",
unemployed = "B27011_008",
employed =	"B27011_003",
foodstamp = "B09010_002",
ilefnhi= "B27011_007",
ilufnhi= "B27011_012",
nilfnhi = "B27011_017",
gini = "B19083_001",
med_income = "B19013_001",
med_2bed = "B25031_004",
single_mom = "B11012_010",
lessthan_hs = "B16010_002",
housing_units = "B25001_001"),
year = 2019)
options(scipen = 0, digits = 3)  # controls number of significant digits printed
library(rpart)         # to train decision trees
library(rpart.plot)    # to plot decision trees
library(randomForest)  # random forests
library(gbm)           # boosting
library(tidyverse)     # tidyverse
library(kableExtra)    # for printing tables
library(cowplot)       # for side by side plots
theftdata=read_csv("../data/clean/dataclean.csv")
set.seed(471) # set seed for reproducibility
train_samples = sample(1:nrow(theftdata), round(0.8*nrow(theftdata)))
theft_train = theftdata %>% filter(row_number() %in% train_samples) %>% select(-county,-state)
theft_test = theftdata %>% filter(!(row_number() %in% train_samples))%>% select(-county,-state)
set.seed(471) # set seed for reproducibility
mvalues = seq(3,15, by = 1)
oob_errors = numeric(length(mvalues))
ntree = 500
for(idx in 1:length(mvalues)){
m = mvalues[idx]
rf_fit = randomForest(theftrate ~ ., mtry = m, data = theft_train)
oob_errors[idx] = rf_fit$mse[ntree]
}
tibble(m = mvalues, oob_err = oob_errors) %>%
ggplot(aes(x = m, y = oob_err)) +
geom_line() + geom_point() +
scale_x_continuous(breaks = mvalues) +
theme_bw()
set.seed(471) # set seed for reproducibility
rf_6 = randomForest(theftrate ~ ., mtry = 6, data = theft_train)
rf_6$importance
varImpPlot(rf_6,n.var = 10)
library(tidyverse)
library(glmnetUtils)
source("/code/functions/plot_glmnet.R")
source("../code/functions/plot_glmnet.R")
View(theftdata)
options(scipen = 0, digits = 3)  # controls number of significant digits printed
library(rpart)         # to train decision trees
library(rpart.plot)    # to plot decision trees
library(randomForest)  # random forests
library(gbm)           # boosting
library(tidyverse)     # tidyverse
library(kableExtra)    # for printing tables
library(cowplot)       # for side by side plots
theftdata=read_csv("../data/clean/dataclean.csv")
set.seed(471) # set seed for reproducibility
train_samples = sample(1:nrow(theftdata), round(0.8*nrow(theftdata)))
theft_train = theftdata %>% filter(row_number() %in% train_samples) %>% select(-county,-state)
theft_test = theftdata %>% filter(!(row_number() %in% train_samples))%>% select(-county,-state)
set.seed(471) # set seed for reproducibility
mvalues = seq(3,15, by = 1)
oob_errors = numeric(length(mvalues))
ntree = 500
for(idx in 1:length(mvalues)){
m = mvalues[idx]
rf_fit = randomForest(theftrate ~ -fips ., mtry = m, data = theft_train)
options(scipen = 0, digits = 3)  # controls number of significant digits printed
library(rpart)         # to train decision trees
library(rpart.plot)    # to plot decision trees
library(randomForest)  # random forests
library(gbm)           # boosting
library(tidyverse)     # tidyverse
library(kableExtra)    # for printing tables
library(cowplot)       # for side by side plots
theftdata=read_csv("../data/clean/dataclean.csv")
set.seed(471) # set seed for reproducibility
train_samples = sample(1:nrow(theftdata), round(0.8*nrow(theftdata)))
theft_train = theftdata %>% filter(row_number() %in% train_samples) %>% select(-county,-state)
theft_test = theftdata %>% filter(!(row_number() %in% train_samples))%>% select(-county,-state)
set.seed(471) # set seed for reproducibility
mvalues = seq(3,15, by = 1)
oob_errors = numeric(length(mvalues))
ntree = 500
for(idx in 1:length(mvalues)){
m = mvalues[idx]
rf_fit = randomForest(theftrate ~. -fips, mtry = m, data = theft_train)
oob_errors[idx] = rf_fit$mse[ntree]
}
tibble(m = mvalues, oob_err = oob_errors) %>%
ggplot(aes(x = m, y = oob_err)) +
geom_line() + geom_point() +
scale_x_continuous(breaks = mvalues) +
theme_bw()
set.seed(471) # set seed for reproducibility
rf_6 = randomForest(theftrate ~ .-fips, mtry = 6, data = theft_train)
rf_6$importance
varImpPlot(rf_6,n.var = 10)
set.seed(471) # set seed for reproducibility
rf_6 = randomForest(theftrate ~ .-fips, mtry = 4, data = theft_train)
rf_6$importance
varImpPlot(rf_6,n.var = 10)
library(tidyverse)
library(glmnetUtils)
source("../code/functions/plot_glmnet.R")
theftdata=read_csv("../data/clean/dataclean.csv")
set.seed(471) # set seed for reproducibility
train_samples = sample(1:nrow(theftdata), round(0.8*nrow(theftdata)))
theft_train = theftdata %>% filter(row_number() %in% train_samples) %>% select(-county,-state)
theft_test = theftdata %>% filter(!(row_number() %in% train_samples))%>% select(-county,-state)
set.seed(471) # set seed for reproducibility
ridge_fit = cv.glmnet(theftrate ~ .-fips,  # formula notation, as usual
alpha = 0,                 # alpha = 0 for ridge
nfolds = 10,               # number of folds
data = theft_train)   # data to run ridge on
plot(ridge_fit)
plot_glmnet(ridge_fit, crime_data_train, features_to_plot = 10)
plot_glmnet(ridge_fit, theft_train, features_to_plot = 10)
plot_glmnet(ridge_fit, theft_train, features_to_plot = 7)
plot_glmnet(ridge_fit, theft_train, features_to_plot = 8)
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
setwd("/Users/diyangchu/Documents/2-grad@Penn/STAT471/theft-crime-2020/code")
library(tidyverse)
library(glmnetUtils)
source("/Users/diyangchu/Documents/2-grad@Penn/STAT471/theft-crime-2020/code/functions/plot_glmnet.R")
theft_train = read_csv("../data/clean/theft_train.csv")
theft_test = read_csv("../data/clean/theft_test.csv")
theft_train_model = theft_train %>% select(-fips, -state, -county, -households)
theft_test_pred = theft_test%>%select(-fips, -state, -county, -households)
set.seed(471) # set seed for reproducibility
ridge_fit = cv.glmnet(theftrate ~ .,  # formula notation, as usual
alpha = 0,                 # alpha = 0 for ridge
nfolds = 10,               # number of folds
data = theft_train_model)   # data to run ridge on
plot(ridge_fit)
plot_glmnet(ridge_fit, theft_train_model, features_to_plot = 8)
ridge_predictions = predict(ridge_fit,
newdata = theft_test_pred,
s = "lambda.1se") %>% as.numeric()
RMSE_ridge = sqrt(mean((ridge_predictions - theft_test_pred$theftrate)^2))
